{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4467364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "VAE模型\n",
    "\n",
    "'''\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hiddens=[16,32,128,256], z_dim=128,image_size=128,ch=3):\n",
    "        # 调用父类方法初始化模块的state\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        prev_ch = ch\n",
    "        modules = []\n",
    "        cur_image_size = image_size\n",
    "        # 编码器 ： [bs,ch, input_dim] => [bs,ch, z_dim]\n",
    "        for cur_ch in hiddens:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(prev_ch,cur_ch,kernel_size=3,stride=2,padding=1), #stride=2 图片每次缩小一半\n",
    "                    nn.BatchNorm2d(cur_ch),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            '''通道数每次卷积X2，图片大小每次 /2\n",
    "                类似UNet的操作\n",
    "            '''\n",
    "            prev_ch = cur_ch\n",
    "            cur_image_size //= 2\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.mean_linear = nn.Linear(prev_ch * cur_image_size * cur_image_size,z_dim)\n",
    "        self.var_linear = nn.Linear(prev_ch * cur_image_size * cur_image_size,z_dim)\n",
    "\n",
    "        # 解码器 ： [bs,ch, z_dim] => [bs,ch, input_dim]\n",
    "        modules = []\n",
    "        #prev_ch = 256\n",
    "        self.decoder_projection = nn.Linear(z_dim,prev_ch * cur_image_size * cur_image_size)\n",
    "        self.decoder_in_chw = (prev_ch, cur_image_size, cur_image_size)\n",
    "        for i , cur_ch in enumerate( hiddens[::-1] ):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                modules.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.ConvTranspose2d(prev_ch,cur_ch,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                        nn.BatchNorm2d(cur_ch),\n",
    "                        nn.ReLU()\n",
    "                    )\n",
    "                )\n",
    "            prev_ch = cur_ch\n",
    "        #(1,256,8,8) 做完反卷积(3次反卷积 分别是256to128 128to32 32to16 ) 变成(1,16,64,64)\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                #先用反卷积把(1,16,64,64) 变成(1，16,128,128)\n",
    "                #再卷积成三通道\n",
    "                nn.ConvTranspose2d(prev_ch,prev_ch,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                nn.BatchNorm2d(cur_ch),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(cur_ch,3,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        向前传播部分, 在model_name(inputs)时自动调用\n",
    "        \"\"\"\n",
    "        # encoder\n",
    "        mu, log_var = self.encode(x)\n",
    "        \n",
    "        # reparameterization trick\n",
    "        sampled_z = self.reparameterization(mu, log_var)\n",
    "        sampled_z = self.decoder_projection(sampled_z)\n",
    "        # reshape\n",
    "        sampled_z = torch.reshape(sampled_z,(-1, *self.decoder_in_chw))\n",
    "        # decoder\n",
    "        #print(sampled_z.shape)\n",
    "        x_hat = self.decode(sampled_z)\n",
    "        #print(x_hat.shape)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        encoding part\n",
    "        :param x: input image\n",
    "        :return: mu and log_var\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x,1) #把 (bs,256,h, w) 压平成 (bs, ...)\n",
    "        mu = self.mean_linear(x)\n",
    "        log_var = self.var_linear(x)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterization(self, mu, log_var): #重参数化采样z\n",
    "        \"\"\"\n",
    "        Given a standard gaussian distribution epsilon ~ N(0,1),\n",
    "        we can sample the random variable z as per z = mu + sigma * epsilon\n",
    "        :param mu:\n",
    "        :param log_var:\n",
    "        :return: sampled z\n",
    "        \"\"\"\n",
    "        sigma = torch.exp(log_var * 0.5) #标准差sigma, 方差的log log_var\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + sigma * eps  # 这里的“*”是点乘的意思\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Given a sampled z, decode it back to image\n",
    "        :param z:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_hat = self.decoder(z)\n",
    "        #x_hat = torch.sigmoid(self.fc5(h))  # 图片数值取值为[0,1]，不宜用ReLU\n",
    "        return x_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b52c7016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前mode: train\n",
      "mapping classes from D:\\jiao\\datasets\\celeba\\train to indexes:{'face': 0}\n",
      "mapping classes from D:\\jiao\\datasets\\celeba\\test to indexes:{'face': 0}\n",
      "number of trainable parameters(M):6.99\n",
      "Resume checkpoint ./output_dir_pretrained/checkpoint-29.pth\n",
      "With optim & sched!\n",
      "Epoch 30\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:24 loss: 15878.5488 (15878.5488) time: 2.6896 data: 2.5512 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15878.5488 (15578.3717) time: 0.3195 data: 0.2861 max mem: 1909\n",
      "Test: Total time: 0:00:03 (0.3568 s / it)\n",
      "loss 15578.372\n",
      "loss on the 276 test images 15578.37\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 30, Step: 99, Loss: 131539.234375, Lr: 0.0001\n",
      "Epoch: 30, Step: 199, Loss: 131774.125, Lr: 0.0001\n",
      "Epoch: 30, Step: 299, Loss: 130722.0, Lr: 0.0001\n",
      "Epoch: 30, Step: 399, Loss: 128219.8125, Lr: 0.0001\n",
      "Epoch: 30, Step: 499, Loss: 129945.359375, Lr: 0.0001\n",
      "Epoch: 30, Step: 599, Loss: 129887.109375, Lr: 0.0001\n",
      "Epoch: 30, Step: 699, Loss: 132284.8125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 31\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:28 loss: 15924.6484 (15924.6484) time: 3.1333 data: 3.0881 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15924.6484 (15560.3408) time: 0.4483 data: 0.4068 max mem: 1909\n",
      "Test: Total time: 0:00:04 (0.4879 s / it)\n",
      "loss 15560.341\n",
      "loss on the 276 test images 15560.34\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 31, Step: 99, Loss: 127410.203125, Lr: 0.0001\n",
      "Epoch: 31, Step: 199, Loss: 136201.765625, Lr: 0.0001\n",
      "Epoch: 31, Step: 299, Loss: 126909.65625, Lr: 0.0001\n",
      "Epoch: 31, Step: 399, Loss: 125471.953125, Lr: 0.0001\n",
      "Epoch: 31, Step: 499, Loss: 125981.2734375, Lr: 0.0001\n",
      "Epoch: 31, Step: 599, Loss: 126864.5625, Lr: 0.0001\n",
      "Epoch: 31, Step: 699, Loss: 125476.953125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 32\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:32 loss: 15870.3213 (15870.3213) time: 3.6076 data: 3.5326 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15746.6904 (15448.8617) time: 0.5196 data: 0.4588 max mem: 1909\n",
      "Test: Total time: 0:00:05 (0.5633 s / it)\n",
      "loss 15448.862\n",
      "loss on the 276 test images 15448.86\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 32, Step: 99, Loss: 131847.234375, Lr: 0.0001\n",
      "Epoch: 32, Step: 199, Loss: 130203.609375, Lr: 0.0001\n",
      "Epoch: 32, Step: 299, Loss: 128129.296875, Lr: 0.0001\n",
      "Epoch: 32, Step: 399, Loss: 127237.2421875, Lr: 0.0001\n",
      "Epoch: 32, Step: 499, Loss: 128307.5546875, Lr: 0.0001\n",
      "Epoch: 32, Step: 599, Loss: 133717.453125, Lr: 0.0001\n",
      "Epoch: 32, Step: 699, Loss: 125296.25, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 33\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:26 loss: 15837.0742 (15837.0742) time: 2.9769 data: 2.9244 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15837.0742 (15483.8242) time: 0.4815 data: 0.4283 max mem: 1909\n",
      "Test: Total time: 0:00:04 (0.5201 s / it)\n",
      "loss 15483.824\n",
      "loss on the 276 test images 15483.82\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 33, Step: 99, Loss: 124853.40625, Lr: 0.0001\n",
      "Epoch: 33, Step: 199, Loss: 129119.1875, Lr: 0.0001\n",
      "Epoch: 33, Step: 299, Loss: 132214.171875, Lr: 0.0001\n",
      "Epoch: 33, Step: 399, Loss: 130510.859375, Lr: 0.0001\n",
      "Epoch: 33, Step: 499, Loss: 123137.8125, Lr: 0.0001\n",
      "Epoch: 33, Step: 599, Loss: 130299.8046875, Lr: 0.0001\n",
      "Epoch: 33, Step: 699, Loss: 131038.578125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 34\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15884.3789 (15884.3789) time: 2.3136 data: 2.2746 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15837.8066 (15466.8870) time: 0.2768 data: 0.2533 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3143 s / it)\n",
      "loss 15466.887\n",
      "loss on the 276 test images 15466.89\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 34, Step: 99, Loss: 132199.84375, Lr: 0.0001\n",
      "Epoch: 34, Step: 199, Loss: 128066.046875, Lr: 0.0001\n",
      "Epoch: 34, Step: 299, Loss: 127964.4921875, Lr: 0.0001\n",
      "Epoch: 34, Step: 399, Loss: 124192.359375, Lr: 0.0001\n",
      "Epoch: 34, Step: 499, Loss: 127208.5625, Lr: 0.0001\n",
      "Epoch: 34, Step: 599, Loss: 127853.65625, Lr: 0.0001\n",
      "Epoch: 34, Step: 699, Loss: 125119.203125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 35\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:21 loss: 15810.0703 (15810.0703) time: 2.3531 data: 2.3141 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15810.0703 (15387.8598) time: 0.2863 data: 0.2594 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3269 s / it)\n",
      "loss 15387.860\n",
      "loss on the 276 test images 15387.86\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 35, Step: 99, Loss: 128762.296875, Lr: 0.0001\n",
      "Epoch: 35, Step: 199, Loss: 133999.96875, Lr: 0.0001\n",
      "Epoch: 35, Step: 299, Loss: 128946.453125, Lr: 0.0001\n",
      "Epoch: 35, Step: 399, Loss: 126744.140625, Lr: 0.0001\n",
      "Epoch: 35, Step: 499, Loss: 124402.375, Lr: 0.0001\n",
      "Epoch: 35, Step: 599, Loss: 130704.46875, Lr: 0.0001\n",
      "Epoch: 35, Step: 699, Loss: 129399.375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 36\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:26 loss: 15795.9961 (15795.9961) time: 2.9924 data: 2.9445 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15773.6230 (15412.6092) time: 0.4407 data: 0.3862 max mem: 1909\n",
      "Test: Total time: 0:00:04 (0.4777 s / it)\n",
      "loss 15412.609\n",
      "loss on the 276 test images 15412.61\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 36, Step: 99, Loss: 126288.46875, Lr: 0.0001\n",
      "Epoch: 36, Step: 199, Loss: 130247.609375, Lr: 0.0001\n",
      "Epoch: 36, Step: 299, Loss: 127384.6875, Lr: 0.0001\n",
      "Epoch: 36, Step: 399, Loss: 125266.375, Lr: 0.0001\n",
      "Epoch: 36, Step: 499, Loss: 126054.3828125, Lr: 0.0001\n",
      "Epoch: 36, Step: 599, Loss: 129623.4453125, Lr: 0.0001\n",
      "Epoch: 36, Step: 699, Loss: 124537.5625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 37\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15721.8379 (15721.8379) time: 2.1367 data: 2.1017 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15702.8164 (15372.4923) time: 0.2657 data: 0.2343 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3020 s / it)\n",
      "loss 15372.492\n",
      "loss on the 276 test images 15372.49\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 37, Step: 99, Loss: 123124.265625, Lr: 0.0001\n",
      "Epoch: 37, Step: 199, Loss: 123577.6328125, Lr: 0.0001\n",
      "Epoch: 37, Step: 299, Loss: 128966.515625, Lr: 0.0001\n",
      "Epoch: 37, Step: 399, Loss: 123401.28125, Lr: 0.0001\n",
      "Epoch: 37, Step: 499, Loss: 126452.7890625, Lr: 0.0001\n",
      "Epoch: 37, Step: 599, Loss: 125905.71875, Lr: 0.0001\n",
      "Epoch: 37, Step: 699, Loss: 128688.265625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 38\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15668.5840 (15668.5840) time: 2.1269 data: 2.0905 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15668.5840 (15336.0216) time: 0.2669 data: 0.2354 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3043 s / it)\n",
      "loss 15336.022\n",
      "loss on the 276 test images 15336.02\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 38, Step: 99, Loss: 128108.453125, Lr: 0.0001\n",
      "Epoch: 38, Step: 199, Loss: 125668.9765625, Lr: 0.0001\n",
      "Epoch: 38, Step: 299, Loss: 128202.5859375, Lr: 0.0001\n",
      "Epoch: 38, Step: 399, Loss: 131399.96875, Lr: 0.0001\n",
      "Epoch: 38, Step: 499, Loss: 127599.8671875, Lr: 0.0001\n",
      "Epoch: 38, Step: 599, Loss: 124924.5703125, Lr: 0.0001\n",
      "Epoch: 38, Step: 699, Loss: 125521.515625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 39\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:21 loss: 15690.1855 (15690.1855) time: 2.3863 data: 2.3446 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15687.0439 (15300.1864) time: 0.2938 data: 0.2631 max mem: 1909\n",
      "Test: Total time: 0:00:03 (0.3353 s / it)\n",
      "loss 15300.186\n",
      "loss on the 276 test images 15300.19\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 39, Step: 99, Loss: 132266.0625, Lr: 0.0001\n",
      "Epoch: 39, Step: 199, Loss: 127750.546875, Lr: 0.0001\n",
      "Epoch: 39, Step: 299, Loss: 132722.953125, Lr: 0.0001\n",
      "Epoch: 39, Step: 399, Loss: 125152.8125, Lr: 0.0001\n",
      "Epoch: 39, Step: 499, Loss: 126072.7109375, Lr: 0.0001\n",
      "Epoch: 39, Step: 599, Loss: 127170.8359375, Lr: 0.0001\n",
      "Epoch: 39, Step: 699, Loss: 126779.5, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 40\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15666.6924 (15666.6924) time: 2.3247 data: 2.2841 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15666.6924 (15338.6492) time: 0.2806 data: 0.2551 max mem: 1909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Total time: 0:00:02 (0.3170 s / it)\n",
      "loss 15338.649\n",
      "loss on the 276 test images 15338.65\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 40, Step: 99, Loss: 125561.171875, Lr: 0.0001\n",
      "Epoch: 40, Step: 199, Loss: 125020.5, Lr: 0.0001\n",
      "Epoch: 40, Step: 299, Loss: 127527.65625, Lr: 0.0001\n",
      "Epoch: 40, Step: 399, Loss: 128023.7890625, Lr: 0.0001\n",
      "Epoch: 40, Step: 499, Loss: 125972.7890625, Lr: 0.0001\n",
      "Epoch: 40, Step: 599, Loss: 124517.90625, Lr: 0.0001\n",
      "Epoch: 40, Step: 699, Loss: 127058.28125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 41\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15671.5449 (15671.5449) time: 2.2441 data: 2.2080 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15558.9062 (15272.3812) time: 0.2756 data: 0.2462 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3149 s / it)\n",
      "loss 15272.381\n",
      "loss on the 276 test images 15272.38\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 41, Step: 99, Loss: 121212.71875, Lr: 0.0001\n",
      "Epoch: 41, Step: 199, Loss: 127700.6640625, Lr: 0.0001\n",
      "Epoch: 41, Step: 299, Loss: 128808.53125, Lr: 0.0001\n",
      "Epoch: 41, Step: 399, Loss: 123180.671875, Lr: 0.0001\n",
      "Epoch: 41, Step: 499, Loss: 127601.296875, Lr: 0.0001\n",
      "Epoch: 41, Step: 599, Loss: 125855.90625, Lr: 0.0001\n",
      "Epoch: 41, Step: 699, Loss: 129236.9375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 42\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:21 loss: 15710.1719 (15710.1719) time: 2.3899 data: 2.3481 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15710.1719 (15334.8427) time: 0.3006 data: 0.2682 max mem: 1909\n",
      "Test: Total time: 0:00:03 (0.3379 s / it)\n",
      "loss 15334.843\n",
      "loss on the 276 test images 15334.84\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 42, Step: 99, Loss: 127699.9921875, Lr: 0.0001\n",
      "Epoch: 42, Step: 199, Loss: 129998.0859375, Lr: 0.0001\n",
      "Epoch: 42, Step: 299, Loss: 124872.6015625, Lr: 0.0001\n",
      "Epoch: 42, Step: 399, Loss: 122610.828125, Lr: 0.0001\n",
      "Epoch: 42, Step: 499, Loss: 122991.546875, Lr: 0.0001\n",
      "Epoch: 42, Step: 599, Loss: 131270.390625, Lr: 0.0001\n",
      "Epoch: 42, Step: 699, Loss: 133038.265625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 43\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15660.4141 (15660.4141) time: 2.2972 data: 2.2652 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15533.1582 (15259.0180) time: 0.2856 data: 0.2588 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3235 s / it)\n",
      "loss 15259.018\n",
      "loss on the 276 test images 15259.02\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 43, Step: 99, Loss: 126324.90625, Lr: 0.0001\n",
      "Epoch: 43, Step: 199, Loss: 126397.0, Lr: 0.0001\n",
      "Epoch: 43, Step: 299, Loss: 124512.96875, Lr: 0.0001\n",
      "Epoch: 43, Step: 399, Loss: 126724.953125, Lr: 0.0001\n",
      "Epoch: 43, Step: 499, Loss: 125217.2265625, Lr: 0.0001\n",
      "Epoch: 43, Step: 599, Loss: 122835.0390625, Lr: 0.0001\n",
      "Epoch: 43, Step: 699, Loss: 131797.09375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 44\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15658.2920 (15658.2920) time: 2.2223 data: 2.1874 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15658.2920 (15264.0094) time: 0.2719 data: 0.2449 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3119 s / it)\n",
      "loss 15264.009\n",
      "loss on the 276 test images 15264.01\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 44, Step: 99, Loss: 128343.7734375, Lr: 0.0001\n",
      "Epoch: 44, Step: 199, Loss: 127995.0234375, Lr: 0.0001\n",
      "Epoch: 44, Step: 299, Loss: 131815.84375, Lr: 0.0001\n",
      "Epoch: 44, Step: 399, Loss: 127270.4453125, Lr: 0.0001\n",
      "Epoch: 44, Step: 499, Loss: 126746.359375, Lr: 0.0001\n",
      "Epoch: 44, Step: 599, Loss: 124219.9453125, Lr: 0.0001\n",
      "Epoch: 44, Step: 699, Loss: 131468.984375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 45\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15570.8379 (15570.8379) time: 2.2420 data: 2.2055 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15570.8379 (15202.1301) time: 0.2767 data: 0.2456 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3156 s / it)\n",
      "loss 15202.130\n",
      "loss on the 276 test images 15202.13\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 45, Step: 99, Loss: 127660.5390625, Lr: 0.0001\n",
      "Epoch: 45, Step: 199, Loss: 122445.375, Lr: 0.0001\n",
      "Epoch: 45, Step: 299, Loss: 127007.34375, Lr: 0.0001\n",
      "Epoch: 45, Step: 399, Loss: 128592.4296875, Lr: 0.0001\n",
      "Epoch: 45, Step: 499, Loss: 123947.9375, Lr: 0.0001\n",
      "Epoch: 45, Step: 599, Loss: 124972.3125, Lr: 0.0001\n",
      "Epoch: 45, Step: 699, Loss: 122744.0390625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 46\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15608.8379 (15608.8379) time: 2.1607 data: 2.1232 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15568.8555 (15175.8951) time: 0.2720 data: 0.2414 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3108 s / it)\n",
      "loss 15175.895\n",
      "loss on the 276 test images 15175.90\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 46, Step: 99, Loss: 128165.8671875, Lr: 0.0001\n",
      "Epoch: 46, Step: 199, Loss: 122767.703125, Lr: 0.0001\n",
      "Epoch: 46, Step: 299, Loss: 124145.734375, Lr: 0.0001\n",
      "Epoch: 46, Step: 399, Loss: 123121.59375, Lr: 0.0001\n",
      "Epoch: 46, Step: 499, Loss: 124259.671875, Lr: 0.0001\n",
      "Epoch: 46, Step: 599, Loss: 127096.6328125, Lr: 0.0001\n",
      "Epoch: 46, Step: 699, Loss: 129304.09375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 47\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15509.5840 (15509.5840) time: 2.2360 data: 2.2006 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15509.5840 (15170.0437) time: 0.2719 data: 0.2464 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3086 s / it)\n",
      "loss 15170.044\n",
      "loss on the 276 test images 15170.04\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 47, Step: 99, Loss: 122625.15625, Lr: 0.0001\n",
      "Epoch: 47, Step: 199, Loss: 125305.8828125, Lr: 0.0001\n",
      "Epoch: 47, Step: 299, Loss: 127059.671875, Lr: 0.0001\n",
      "Epoch: 47, Step: 399, Loss: 122630.03125, Lr: 0.0001\n",
      "Epoch: 47, Step: 499, Loss: 128708.3671875, Lr: 0.0001\n",
      "Epoch: 47, Step: 599, Loss: 126135.359375, Lr: 0.0001\n",
      "Epoch: 47, Step: 699, Loss: 126691.046875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 48\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15651.6221 (15651.6221) time: 2.1979 data: 2.1606 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15517.9668 (15214.2985) time: 0.2725 data: 0.2432 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3104 s / it)\n",
      "loss 15214.299\n",
      "loss on the 276 test images 15214.30\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 48, Step: 99, Loss: 126946.546875, Lr: 0.0001\n",
      "Epoch: 48, Step: 199, Loss: 128703.1953125, Lr: 0.0001\n",
      "Epoch: 48, Step: 299, Loss: 125008.203125, Lr: 0.0001\n",
      "Epoch: 48, Step: 399, Loss: 126734.0859375, Lr: 0.0001\n",
      "Epoch: 48, Step: 499, Loss: 124511.125, Lr: 0.0001\n",
      "Epoch: 48, Step: 599, Loss: 124998.296875, Lr: 0.0001\n",
      "Epoch: 48, Step: 699, Loss: 126940.890625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 49\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15543.8867 (15543.8867) time: 2.1918 data: 2.1555 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15540.7939 (15157.2845) time: 0.2673 data: 0.2412 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3065 s / it)\n",
      "loss 15157.285\n",
      "loss on the 276 test images 15157.28\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 49, Step: 99, Loss: 127910.6953125, Lr: 0.0001\n",
      "Epoch: 49, Step: 199, Loss: 119596.890625, Lr: 0.0001\n",
      "Epoch: 49, Step: 299, Loss: 126449.015625, Lr: 0.0001\n",
      "Epoch: 49, Step: 399, Loss: 124797.1640625, Lr: 0.0001\n",
      "Epoch: 49, Step: 499, Loss: 127545.4140625, Lr: 0.0001\n",
      "Epoch: 49, Step: 599, Loss: 129928.96875, Lr: 0.0001\n",
      "Epoch: 49, Step: 699, Loss: 126291.625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 50\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15661.4072 (15661.4072) time: 2.1791 data: 2.1409 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15611.3340 (15159.2662) time: 0.2643 data: 0.2390 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3037 s / it)\n",
      "loss 15159.266\n",
      "loss on the 276 test images 15159.27\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 50, Step: 99, Loss: 124568.5, Lr: 0.0001\n",
      "Epoch: 50, Step: 199, Loss: 132342.6875, Lr: 0.0001\n",
      "Epoch: 50, Step: 299, Loss: 121488.4609375, Lr: 0.0001\n",
      "Epoch: 50, Step: 399, Loss: 127110.4453125, Lr: 0.0001\n",
      "Epoch: 50, Step: 499, Loss: 126422.203125, Lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 50, Step: 599, Loss: 127942.1328125, Lr: 0.0001\n",
      "Epoch: 50, Step: 699, Loss: 121656.96875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 51\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15668.3652 (15668.3652) time: 2.2441 data: 2.2059 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15575.5508 (15168.4307) time: 0.2760 data: 0.2453 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3151 s / it)\n",
      "loss 15168.431\n",
      "loss on the 276 test images 15168.43\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 51, Step: 99, Loss: 127071.546875, Lr: 0.0001\n",
      "Epoch: 51, Step: 199, Loss: 127828.1015625, Lr: 0.0001\n",
      "Epoch: 51, Step: 299, Loss: 127422.15625, Lr: 0.0001\n",
      "Epoch: 51, Step: 399, Loss: 126645.34375, Lr: 0.0001\n",
      "Epoch: 51, Step: 499, Loss: 123924.6953125, Lr: 0.0001\n",
      "Epoch: 51, Step: 599, Loss: 124026.15625, Lr: 0.0001\n",
      "Epoch: 51, Step: 699, Loss: 125142.296875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 52\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:21 loss: 15503.3789 (15503.3789) time: 2.3910 data: 2.3493 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15503.3789 (15117.2483) time: 0.3016 data: 0.2678 max mem: 1909\n",
      "Test: Total time: 0:00:03 (0.3419 s / it)\n",
      "loss 15117.248\n",
      "loss on the 276 test images 15117.25\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 52, Step: 99, Loss: 123813.890625, Lr: 0.0001\n",
      "Epoch: 52, Step: 199, Loss: 126255.59375, Lr: 0.0001\n",
      "Epoch: 52, Step: 299, Loss: 124789.078125, Lr: 0.0001\n",
      "Epoch: 52, Step: 399, Loss: 124860.7109375, Lr: 0.0001\n",
      "Epoch: 52, Step: 499, Loss: 128821.7890625, Lr: 0.0001\n",
      "Epoch: 52, Step: 599, Loss: 121388.203125, Lr: 0.0001\n",
      "Epoch: 52, Step: 699, Loss: 128358.28125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 53\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15797.0459 (15797.0459) time: 2.1640 data: 2.1321 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15690.4922 (15270.7249) time: 0.2690 data: 0.2409 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3049 s / it)\n",
      "loss 15270.725\n",
      "loss on the 276 test images 15270.72\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 53, Step: 99, Loss: 121627.5390625, Lr: 0.0001\n",
      "Epoch: 53, Step: 199, Loss: 122741.171875, Lr: 0.0001\n",
      "Epoch: 53, Step: 299, Loss: 126107.7890625, Lr: 0.0001\n",
      "Epoch: 53, Step: 399, Loss: 120829.78125, Lr: 0.0001\n",
      "Epoch: 53, Step: 499, Loss: 125170.65625, Lr: 0.0001\n",
      "Epoch: 53, Step: 599, Loss: 128697.125, Lr: 0.0001\n",
      "Epoch: 53, Step: 699, Loss: 123807.953125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 54\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15620.8896 (15620.8896) time: 2.1738 data: 2.1384 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15486.3740 (15146.2741) time: 0.2723 data: 0.2396 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3102 s / it)\n",
      "loss 15146.274\n",
      "loss on the 276 test images 15146.27\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 54, Step: 99, Loss: 129590.1875, Lr: 0.0001\n",
      "Epoch: 54, Step: 199, Loss: 123254.9609375, Lr: 0.0001\n",
      "Epoch: 54, Step: 299, Loss: 124449.7890625, Lr: 0.0001\n",
      "Epoch: 54, Step: 399, Loss: 121232.4140625, Lr: 0.0001\n",
      "Epoch: 54, Step: 499, Loss: 119978.28125, Lr: 0.0001\n",
      "Epoch: 54, Step: 599, Loss: 119927.0703125, Lr: 0.0001\n",
      "Epoch: 54, Step: 699, Loss: 124575.015625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 55\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15501.6006 (15501.6006) time: 2.2097 data: 2.1714 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15501.6006 (15142.2693) time: 0.2760 data: 0.2423 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3145 s / it)\n",
      "loss 15142.269\n",
      "loss on the 276 test images 15142.27\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 55, Step: 99, Loss: 121012.6328125, Lr: 0.0001\n",
      "Epoch: 55, Step: 199, Loss: 124142.1875, Lr: 0.0001\n",
      "Epoch: 55, Step: 299, Loss: 126104.46875, Lr: 0.0001\n",
      "Epoch: 55, Step: 399, Loss: 120852.0546875, Lr: 0.0001\n",
      "Epoch: 55, Step: 499, Loss: 122428.15625, Lr: 0.0001\n",
      "Epoch: 55, Step: 599, Loss: 124270.484375, Lr: 0.0001\n",
      "Epoch: 55, Step: 699, Loss: 125046.6875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 56\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15511.7012 (15511.7012) time: 2.1510 data: 2.1152 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15511.7012 (15126.2030) time: 0.2639 data: 0.2368 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3036 s / it)\n",
      "loss 15126.203\n",
      "loss on the 276 test images 15126.20\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 56, Step: 99, Loss: 123320.625, Lr: 0.0001\n",
      "Epoch: 56, Step: 199, Loss: 121825.71875, Lr: 0.0001\n",
      "Epoch: 56, Step: 299, Loss: 122599.28125, Lr: 0.0001\n",
      "Epoch: 56, Step: 399, Loss: 125342.8046875, Lr: 0.0001\n",
      "Epoch: 56, Step: 499, Loss: 119478.4453125, Lr: 0.0001\n",
      "Epoch: 56, Step: 599, Loss: 125093.296875, Lr: 0.0001\n",
      "Epoch: 56, Step: 699, Loss: 125274.2734375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 57\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15460.9707 (15460.9707) time: 2.2091 data: 2.1713 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15460.9707 (15149.8185) time: 0.2799 data: 0.2469 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3182 s / it)\n",
      "loss 15149.818\n",
      "loss on the 276 test images 15149.82\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 57, Step: 99, Loss: 121108.46875, Lr: 0.0001\n",
      "Epoch: 57, Step: 199, Loss: 121609.953125, Lr: 0.0001\n",
      "Epoch: 57, Step: 299, Loss: 121977.296875, Lr: 0.0001\n",
      "Epoch: 57, Step: 399, Loss: 123362.15625, Lr: 0.0001\n",
      "Epoch: 57, Step: 499, Loss: 120762.9765625, Lr: 0.0001\n",
      "Epoch: 57, Step: 599, Loss: 125238.359375, Lr: 0.0001\n",
      "Epoch: 57, Step: 699, Loss: 124946.453125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 58\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:21 loss: 15519.9766 (15519.9766) time: 2.4072 data: 2.3662 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15329.7266 (15056.4397) time: 0.2924 data: 0.2637 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3306 s / it)\n",
      "loss 15056.440\n",
      "loss on the 276 test images 15056.44\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 58, Step: 99, Loss: 128139.828125, Lr: 0.0001\n",
      "Epoch: 58, Step: 199, Loss: 121707.890625, Lr: 0.0001\n",
      "Epoch: 58, Step: 299, Loss: 125134.1796875, Lr: 0.0001\n",
      "Epoch: 58, Step: 399, Loss: 132253.265625, Lr: 0.0001\n",
      "Epoch: 58, Step: 499, Loss: 120955.328125, Lr: 0.0001\n",
      "Epoch: 58, Step: 599, Loss: 123918.96875, Lr: 0.0001\n",
      "Epoch: 58, Step: 699, Loss: 125201.1875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 59\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15495.0830 (15495.0830) time: 2.1708 data: 2.1356 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15495.0830 (15129.4022) time: 0.2725 data: 0.2438 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3099 s / it)\n",
      "loss 15129.402\n",
      "loss on the 276 test images 15129.40\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 59, Step: 99, Loss: 128665.890625, Lr: 0.0001\n",
      "Epoch: 59, Step: 199, Loss: 118159.59375, Lr: 0.0001\n",
      "Epoch: 59, Step: 299, Loss: 125100.75, Lr: 0.0001\n",
      "Epoch: 59, Step: 399, Loss: 125337.7421875, Lr: 0.0001\n",
      "Epoch: 59, Step: 499, Loss: 124875.4140625, Lr: 0.0001\n",
      "Epoch: 59, Step: 599, Loss: 120747.8984375, Lr: 0.0001\n",
      "Epoch: 59, Step: 699, Loss: 127396.046875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 60\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:32 loss: 15412.8652 (15412.8652) time: 3.5672 data: 3.4920 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15412.8652 (15003.8056) time: 0.4188 data: 0.3911 max mem: 1909\n",
      "Test: Total time: 0:00:04 (0.4611 s / it)\n",
      "loss 15003.806\n",
      "loss on the 276 test images 15003.81\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 60, Step: 99, Loss: 125060.4296875, Lr: 0.0001\n",
      "Epoch: 60, Step: 199, Loss: 126654.1015625, Lr: 0.0001\n",
      "Epoch: 60, Step: 299, Loss: 123930.171875, Lr: 0.0001\n",
      "Epoch: 60, Step: 399, Loss: 126204.609375, Lr: 0.0001\n",
      "Epoch: 60, Step: 499, Loss: 128317.578125, Lr: 0.0001\n",
      "Epoch: 60, Step: 599, Loss: 122458.71875, Lr: 0.0001\n",
      "Epoch: 60, Step: 699, Loss: 130975.0, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 61\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15383.9717 (15383.9717) time: 2.2250 data: 2.1901 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15383.9717 (15029.3454) time: 0.2740 data: 0.2445 max mem: 1909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Total time: 0:00:02 (0.3113 s / it)\n",
      "loss 15029.345\n",
      "loss on the 276 test images 15029.35\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 61, Step: 99, Loss: 126513.265625, Lr: 0.0001\n",
      "Epoch: 61, Step: 199, Loss: 119752.2578125, Lr: 0.0001\n",
      "Epoch: 61, Step: 299, Loss: 121836.875, Lr: 0.0001\n",
      "Epoch: 61, Step: 399, Loss: 127321.9453125, Lr: 0.0001\n",
      "Epoch: 61, Step: 499, Loss: 123279.9375, Lr: 0.0001\n",
      "Epoch: 61, Step: 599, Loss: 133548.78125, Lr: 0.0001\n",
      "Epoch: 61, Step: 699, Loss: 123760.328125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 62\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15587.3154 (15587.3154) time: 2.3263 data: 2.2887 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15485.7148 (15063.4454) time: 0.2871 data: 0.2552 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3256 s / it)\n",
      "loss 15063.445\n",
      "loss on the 276 test images 15063.45\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 62, Step: 99, Loss: 124972.8984375, Lr: 0.0001\n",
      "Epoch: 62, Step: 199, Loss: 124841.109375, Lr: 0.0001\n",
      "Epoch: 62, Step: 299, Loss: 127176.8125, Lr: 0.0001\n",
      "Epoch: 62, Step: 399, Loss: 124267.9375, Lr: 0.0001\n",
      "Epoch: 62, Step: 499, Loss: 122143.5625, Lr: 0.0001\n",
      "Epoch: 62, Step: 599, Loss: 128064.0234375, Lr: 0.0001\n",
      "Epoch: 62, Step: 699, Loss: 133618.6875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 63\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15550.9717 (15550.9717) time: 2.1743 data: 2.1379 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15431.6973 (15060.6584) time: 0.2644 data: 0.2384 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3018 s / it)\n",
      "loss 15060.658\n",
      "loss on the 276 test images 15060.66\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 63, Step: 99, Loss: 126542.703125, Lr: 0.0001\n",
      "Epoch: 63, Step: 199, Loss: 126865.015625, Lr: 0.0001\n",
      "Epoch: 63, Step: 299, Loss: 126514.234375, Lr: 0.0001\n",
      "Epoch: 63, Step: 399, Loss: 120672.28125, Lr: 0.0001\n",
      "Epoch: 63, Step: 499, Loss: 124179.6875, Lr: 0.0001\n",
      "Epoch: 63, Step: 599, Loss: 122297.1171875, Lr: 0.0001\n",
      "Epoch: 63, Step: 699, Loss: 119282.3125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 64\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15661.9521 (15661.9521) time: 2.2319 data: 2.1947 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15542.7842 (15106.2650) time: 0.2764 data: 0.2479 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3150 s / it)\n",
      "loss 15106.265\n",
      "loss on the 276 test images 15106.26\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 64, Step: 99, Loss: 124481.140625, Lr: 0.0001\n",
      "Epoch: 64, Step: 199, Loss: 126101.9921875, Lr: 0.0001\n",
      "Epoch: 64, Step: 299, Loss: 123326.9765625, Lr: 0.0001\n",
      "Epoch: 64, Step: 399, Loss: 125699.453125, Lr: 0.0001\n",
      "Epoch: 64, Step: 499, Loss: 129350.578125, Lr: 0.0001\n",
      "Epoch: 64, Step: 599, Loss: 121423.34375, Lr: 0.0001\n",
      "Epoch: 64, Step: 699, Loss: 125240.1953125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 65\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15373.8467 (15373.8467) time: 2.1858 data: 2.1548 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15373.8467 (14991.1944) time: 0.2705 data: 0.2419 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3090 s / it)\n",
      "loss 14991.194\n",
      "loss on the 276 test images 14991.19\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 65, Step: 99, Loss: 121790.3125, Lr: 0.0001\n",
      "Epoch: 65, Step: 199, Loss: 122202.2890625, Lr: 0.0001\n",
      "Epoch: 65, Step: 299, Loss: 126162.5, Lr: 0.0001\n",
      "Epoch: 65, Step: 399, Loss: 122624.890625, Lr: 0.0001\n",
      "Epoch: 65, Step: 499, Loss: 125714.78125, Lr: 0.0001\n",
      "Epoch: 65, Step: 599, Loss: 124514.65625, Lr: 0.0001\n",
      "Epoch: 65, Step: 699, Loss: 127189.03125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 66\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15551.9541 (15551.9541) time: 2.1628 data: 2.1288 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15528.5215 (15059.2964) time: 0.2682 data: 0.2381 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3069 s / it)\n",
      "loss 15059.296\n",
      "loss on the 276 test images 15059.30\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 66, Step: 99, Loss: 123467.3515625, Lr: 0.0001\n",
      "Epoch: 66, Step: 199, Loss: 125326.28125, Lr: 0.0001\n",
      "Epoch: 66, Step: 299, Loss: 119325.84375, Lr: 0.0001\n",
      "Epoch: 66, Step: 399, Loss: 120582.7109375, Lr: 0.0001\n",
      "Epoch: 66, Step: 499, Loss: 124455.578125, Lr: 0.0001\n",
      "Epoch: 66, Step: 599, Loss: 123983.9609375, Lr: 0.0001\n",
      "Epoch: 66, Step: 699, Loss: 124565.140625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 67\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15414.3164 (15414.3164) time: 2.2615 data: 2.2235 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15346.9102 (14999.5704) time: 0.2781 data: 0.2477 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3169 s / it)\n",
      "loss 14999.570\n",
      "loss on the 276 test images 14999.57\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 67, Step: 99, Loss: 122520.421875, Lr: 0.0001\n",
      "Epoch: 67, Step: 199, Loss: 126860.015625, Lr: 0.0001\n",
      "Epoch: 67, Step: 299, Loss: 126933.6953125, Lr: 0.0001\n",
      "Epoch: 67, Step: 399, Loss: 124019.484375, Lr: 0.0001\n",
      "Epoch: 67, Step: 499, Loss: 125110.3828125, Lr: 0.0001\n",
      "Epoch: 67, Step: 599, Loss: 123661.65625, Lr: 0.0001\n",
      "Epoch: 67, Step: 699, Loss: 121349.234375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 68\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15387.3223 (15387.3223) time: 2.1257 data: 2.0893 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15261.1553 (14958.0018) time: 0.2621 data: 0.2344 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2994 s / it)\n",
      "loss 14958.002\n",
      "loss on the 276 test images 14958.00\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 68, Step: 99, Loss: 119215.0390625, Lr: 0.0001\n",
      "Epoch: 68, Step: 199, Loss: 126546.59375, Lr: 0.0001\n",
      "Epoch: 68, Step: 299, Loss: 125562.3203125, Lr: 0.0001\n",
      "Epoch: 68, Step: 399, Loss: 122170.875, Lr: 0.0001\n",
      "Epoch: 68, Step: 499, Loss: 126053.8984375, Lr: 0.0001\n",
      "Epoch: 68, Step: 599, Loss: 118441.0703125, Lr: 0.0001\n",
      "Epoch: 68, Step: 699, Loss: 123486.90625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 69\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15369.4775 (15369.4775) time: 2.1135 data: 2.0784 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15369.4775 (15033.1648) time: 0.2597 data: 0.2322 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2951 s / it)\n",
      "loss 15033.165\n",
      "loss on the 276 test images 15033.16\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 69, Step: 99, Loss: 129250.0703125, Lr: 0.0001\n",
      "Epoch: 69, Step: 199, Loss: 122531.6171875, Lr: 0.0001\n",
      "Epoch: 69, Step: 299, Loss: 121411.046875, Lr: 0.0001\n",
      "Epoch: 69, Step: 399, Loss: 122066.171875, Lr: 0.0001\n",
      "Epoch: 69, Step: 499, Loss: 127987.1796875, Lr: 0.0001\n",
      "Epoch: 69, Step: 599, Loss: 122673.265625, Lr: 0.0001\n",
      "Epoch: 69, Step: 699, Loss: 125321.96875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 70\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15241.7480 (15241.7480) time: 2.1789 data: 2.1463 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15241.7480 (14955.7486) time: 0.2696 data: 0.2402 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3079 s / it)\n",
      "loss 14955.749\n",
      "loss on the 276 test images 14955.75\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 70, Step: 99, Loss: 122625.296875, Lr: 0.0001\n",
      "Epoch: 70, Step: 199, Loss: 124280.46875, Lr: 0.0001\n",
      "Epoch: 70, Step: 299, Loss: 121589.71875, Lr: 0.0001\n",
      "Epoch: 70, Step: 399, Loss: 124606.3046875, Lr: 0.0001\n",
      "Epoch: 70, Step: 499, Loss: 124652.21875, Lr: 0.0001\n",
      "Epoch: 70, Step: 599, Loss: 117284.0546875, Lr: 0.0001\n",
      "Epoch: 70, Step: 699, Loss: 121326.7734375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 71\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15371.6494 (15371.6494) time: 2.1616 data: 2.1255 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15224.9707 (14923.2969) time: 0.2649 data: 0.2389 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3006 s / it)\n",
      "loss 14923.297\n",
      "loss on the 276 test images 14923.30\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 71, Step: 99, Loss: 126902.296875, Lr: 0.0001\n",
      "Epoch: 71, Step: 199, Loss: 127149.953125, Lr: 0.0001\n",
      "Epoch: 71, Step: 299, Loss: 120149.3359375, Lr: 0.0001\n",
      "Epoch: 71, Step: 399, Loss: 121499.3671875, Lr: 0.0001\n",
      "Epoch: 71, Step: 499, Loss: 123903.546875, Lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 71, Step: 599, Loss: 122707.890625, Lr: 0.0001\n",
      "Epoch: 71, Step: 699, Loss: 126105.96875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 72\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15332.3047 (15332.3047) time: 2.1126 data: 2.0840 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15332.3047 (14943.0770) time: 0.2655 data: 0.2370 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3029 s / it)\n",
      "loss 14943.077\n",
      "loss on the 276 test images 14943.08\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 72, Step: 99, Loss: 126707.7109375, Lr: 0.0001\n",
      "Epoch: 72, Step: 199, Loss: 125803.59375, Lr: 0.0001\n",
      "Epoch: 72, Step: 299, Loss: 119833.9609375, Lr: 0.0001\n",
      "Epoch: 72, Step: 399, Loss: 120211.96875, Lr: 0.0001\n",
      "Epoch: 72, Step: 499, Loss: 119623.9765625, Lr: 0.0001\n",
      "Epoch: 72, Step: 599, Loss: 125135.0625, Lr: 0.0001\n",
      "Epoch: 72, Step: 699, Loss: 131045.265625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 73\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15557.5186 (15557.5186) time: 2.1359 data: 2.1016 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15314.0264 (14997.5124) time: 0.2654 data: 0.2364 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3032 s / it)\n",
      "loss 14997.512\n",
      "loss on the 276 test images 14997.51\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 73, Step: 99, Loss: 122441.59375, Lr: 0.0001\n",
      "Epoch: 73, Step: 199, Loss: 119905.953125, Lr: 0.0001\n",
      "Epoch: 73, Step: 299, Loss: 121215.25, Lr: 0.0001\n",
      "Epoch: 73, Step: 399, Loss: 122675.6015625, Lr: 0.0001\n",
      "Epoch: 73, Step: 499, Loss: 124418.8125, Lr: 0.0001\n",
      "Epoch: 73, Step: 599, Loss: 123137.9375, Lr: 0.0001\n",
      "Epoch: 73, Step: 699, Loss: 121336.859375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 74\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15416.6465 (15416.6465) time: 2.1781 data: 2.1410 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15294.6680 (14927.9817) time: 0.2676 data: 0.2391 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3052 s / it)\n",
      "loss 14927.982\n",
      "loss on the 276 test images 14927.98\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 74, Step: 99, Loss: 121978.109375, Lr: 0.0001\n",
      "Epoch: 74, Step: 199, Loss: 124552.921875, Lr: 0.0001\n",
      "Epoch: 74, Step: 299, Loss: 128122.8125, Lr: 0.0001\n",
      "Epoch: 74, Step: 399, Loss: 124227.703125, Lr: 0.0001\n",
      "Epoch: 74, Step: 499, Loss: 124919.40625, Lr: 0.0001\n",
      "Epoch: 74, Step: 599, Loss: 126440.078125, Lr: 0.0001\n",
      "Epoch: 74, Step: 699, Loss: 125653.5625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 75\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15322.8848 (15322.8848) time: 2.1543 data: 2.1187 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15288.8789 (14948.7643) time: 0.2613 data: 0.2378 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2989 s / it)\n",
      "loss 14948.764\n",
      "loss on the 276 test images 14948.76\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 75, Step: 99, Loss: 123651.71875, Lr: 0.0001\n",
      "Epoch: 75, Step: 199, Loss: 122842.5078125, Lr: 0.0001\n",
      "Epoch: 75, Step: 299, Loss: 120307.4140625, Lr: 0.0001\n",
      "Epoch: 75, Step: 399, Loss: 124032.640625, Lr: 0.0001\n",
      "Epoch: 75, Step: 499, Loss: 123566.8515625, Lr: 0.0001\n",
      "Epoch: 75, Step: 599, Loss: 121504.28125, Lr: 0.0001\n",
      "Epoch: 75, Step: 699, Loss: 119740.546875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 76\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15441.8008 (15441.8008) time: 2.1411 data: 2.1055 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15350.3359 (14970.6764) time: 0.2640 data: 0.2355 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3015 s / it)\n",
      "loss 14970.676\n",
      "loss on the 276 test images 14970.68\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 76, Step: 99, Loss: 123338.265625, Lr: 0.0001\n",
      "Epoch: 76, Step: 199, Loss: 121427.484375, Lr: 0.0001\n",
      "Epoch: 76, Step: 299, Loss: 124139.53125, Lr: 0.0001\n",
      "Epoch: 76, Step: 399, Loss: 124186.8125, Lr: 0.0001\n",
      "Epoch: 76, Step: 499, Loss: 121848.8359375, Lr: 0.0001\n",
      "Epoch: 76, Step: 599, Loss: 123679.953125, Lr: 0.0001\n",
      "Epoch: 76, Step: 699, Loss: 119903.34375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 77\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15369.6465 (15369.6465) time: 2.1755 data: 2.1433 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15328.1270 (14924.3347) time: 0.2666 data: 0.2392 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3028 s / it)\n",
      "loss 14924.335\n",
      "loss on the 276 test images 14924.33\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 77, Step: 99, Loss: 121706.734375, Lr: 0.0001\n",
      "Epoch: 77, Step: 199, Loss: 121633.4375, Lr: 0.0001\n",
      "Epoch: 77, Step: 299, Loss: 121796.4140625, Lr: 0.0001\n",
      "Epoch: 77, Step: 399, Loss: 124994.5703125, Lr: 0.0001\n",
      "Epoch: 77, Step: 499, Loss: 123623.890625, Lr: 0.0001\n",
      "Epoch: 77, Step: 599, Loss: 123197.28125, Lr: 0.0001\n",
      "Epoch: 77, Step: 699, Loss: 120643.7734375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 78\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15359.4229 (15359.4229) time: 2.1770 data: 2.1388 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15359.4229 (14932.1440) time: 0.2640 data: 0.2380 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3015 s / it)\n",
      "loss 14932.144\n",
      "loss on the 276 test images 14932.14\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 78, Step: 99, Loss: 122483.796875, Lr: 0.0001\n",
      "Epoch: 78, Step: 199, Loss: 120121.0, Lr: 0.0001\n",
      "Epoch: 78, Step: 299, Loss: 123951.671875, Lr: 0.0001\n",
      "Epoch: 78, Step: 399, Loss: 124714.1328125, Lr: 0.0001\n",
      "Epoch: 78, Step: 499, Loss: 117739.0, Lr: 0.0001\n",
      "Epoch: 78, Step: 599, Loss: 127967.609375, Lr: 0.0001\n",
      "Epoch: 78, Step: 699, Loss: 121167.3125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 79\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15275.9902 (15275.9902) time: 2.1350 data: 2.1040 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15212.1699 (14891.8142) time: 0.2668 data: 0.2357 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3043 s / it)\n",
      "loss 14891.814\n",
      "loss on the 276 test images 14891.81\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 79, Step: 99, Loss: 131340.875, Lr: 0.0001\n",
      "Epoch: 79, Step: 199, Loss: 123056.65625, Lr: 0.0001\n",
      "Epoch: 79, Step: 299, Loss: 123143.734375, Lr: 0.0001\n",
      "Epoch: 79, Step: 399, Loss: 124731.984375, Lr: 0.0001\n",
      "Epoch: 79, Step: 499, Loss: 124426.640625, Lr: 0.0001\n",
      "Epoch: 79, Step: 599, Loss: 121327.046875, Lr: 0.0001\n",
      "Epoch: 79, Step: 699, Loss: 124370.921875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 80\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15361.8672 (15361.8672) time: 2.2064 data: 2.1720 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15265.4648 (14895.3179) time: 0.2702 data: 0.2438 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3072 s / it)\n",
      "loss 14895.318\n",
      "loss on the 276 test images 14895.32\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 80, Step: 99, Loss: 124321.421875, Lr: 0.0001\n",
      "Epoch: 80, Step: 199, Loss: 121115.109375, Lr: 0.0001\n",
      "Epoch: 80, Step: 299, Loss: 123051.96875, Lr: 0.0001\n",
      "Epoch: 80, Step: 399, Loss: 121973.3828125, Lr: 0.0001\n",
      "Epoch: 80, Step: 499, Loss: 125400.734375, Lr: 0.0001\n",
      "Epoch: 80, Step: 599, Loss: 121533.8203125, Lr: 0.0001\n",
      "Epoch: 80, Step: 699, Loss: 126731.078125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 81\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:18 loss: 15271.6436 (15271.6436) time: 2.1101 data: 2.0750 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15271.6436 (14882.3805) time: 0.2609 data: 0.2320 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2974 s / it)\n",
      "loss 14882.381\n",
      "loss on the 276 test images 14882.38\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 81, Step: 99, Loss: 122466.078125, Lr: 0.0001\n",
      "Epoch: 81, Step: 199, Loss: 118618.6875, Lr: 0.0001\n",
      "Epoch: 81, Step: 299, Loss: 128728.3046875, Lr: 0.0001\n",
      "Epoch: 81, Step: 399, Loss: 125660.6328125, Lr: 0.0001\n",
      "Epoch: 81, Step: 499, Loss: 121202.484375, Lr: 0.0001\n",
      "Epoch: 81, Step: 599, Loss: 121019.5, Lr: 0.0001\n",
      "Epoch: 81, Step: 699, Loss: 119191.9765625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 82\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15322.1582 (15322.1582) time: 2.1610 data: 2.1241 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15271.3076 (14896.9628) time: 0.2670 data: 0.2386 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3051 s / it)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 14896.963\n",
      "loss on the 276 test images 14896.96\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 82, Step: 99, Loss: 119094.65625, Lr: 0.0001\n",
      "Epoch: 82, Step: 199, Loss: 123080.6875, Lr: 0.0001\n",
      "Epoch: 82, Step: 299, Loss: 127345.9453125, Lr: 0.0001\n",
      "Epoch: 82, Step: 399, Loss: 124976.0703125, Lr: 0.0001\n",
      "Epoch: 82, Step: 499, Loss: 123864.1875, Lr: 0.0001\n",
      "Epoch: 82, Step: 599, Loss: 124634.609375, Lr: 0.0001\n",
      "Epoch: 82, Step: 699, Loss: 123172.5625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 83\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15261.1611 (15261.1611) time: 2.2620 data: 2.2259 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15261.1611 (14959.8992) time: 0.2732 data: 0.2477 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3097 s / it)\n",
      "loss 14959.899\n",
      "loss on the 276 test images 14959.90\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 83, Step: 99, Loss: 122405.953125, Lr: 0.0001\n",
      "Epoch: 83, Step: 199, Loss: 122104.9296875, Lr: 0.0001\n",
      "Epoch: 83, Step: 299, Loss: 122671.8984375, Lr: 0.0001\n",
      "Epoch: 83, Step: 399, Loss: 122653.09375, Lr: 0.0001\n",
      "Epoch: 83, Step: 499, Loss: 129785.5234375, Lr: 0.0001\n",
      "Epoch: 83, Step: 599, Loss: 122314.609375, Lr: 0.0001\n",
      "Epoch: 83, Step: 699, Loss: 123029.515625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 84\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15411.5234 (15411.5234) time: 2.1858 data: 2.1501 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15319.1592 (14864.2385) time: 0.2732 data: 0.2414 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3117 s / it)\n",
      "loss 14864.238\n",
      "loss on the 276 test images 14864.24\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 84, Step: 99, Loss: 122104.09375, Lr: 0.0001\n",
      "Epoch: 84, Step: 199, Loss: 122501.453125, Lr: 0.0001\n",
      "Epoch: 84, Step: 299, Loss: 124810.953125, Lr: 0.0001\n",
      "Epoch: 84, Step: 399, Loss: 128058.1015625, Lr: 0.0001\n",
      "Epoch: 84, Step: 499, Loss: 126517.765625, Lr: 0.0001\n",
      "Epoch: 84, Step: 599, Loss: 126240.5703125, Lr: 0.0001\n",
      "Epoch: 84, Step: 699, Loss: 125476.40625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 85\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15407.5059 (15407.5059) time: 2.1429 data: 2.1070 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15286.3262 (14874.2319) time: 0.2644 data: 0.2360 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3027 s / it)\n",
      "loss 14874.232\n",
      "loss on the 276 test images 14874.23\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 85, Step: 99, Loss: 123959.2890625, Lr: 0.0001\n",
      "Epoch: 85, Step: 199, Loss: 118819.953125, Lr: 0.0001\n",
      "Epoch: 85, Step: 299, Loss: 122933.890625, Lr: 0.0001\n",
      "Epoch: 85, Step: 399, Loss: 121361.328125, Lr: 0.0001\n",
      "Epoch: 85, Step: 499, Loss: 122949.8984375, Lr: 0.0001\n",
      "Epoch: 85, Step: 599, Loss: 127636.90625, Lr: 0.0001\n",
      "Epoch: 85, Step: 699, Loss: 121033.609375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 86\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15277.3887 (15277.3887) time: 2.1445 data: 2.1134 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15277.3887 (14944.4244) time: 0.2664 data: 0.2357 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3031 s / it)\n",
      "loss 14944.424\n",
      "loss on the 276 test images 14944.42\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 86, Step: 99, Loss: 123114.3671875, Lr: 0.0001\n",
      "Epoch: 86, Step: 199, Loss: 119774.03125, Lr: 0.0001\n",
      "Epoch: 86, Step: 299, Loss: 120504.65625, Lr: 0.0001\n",
      "Epoch: 86, Step: 399, Loss: 120796.390625, Lr: 0.0001\n",
      "Epoch: 86, Step: 499, Loss: 121363.671875, Lr: 0.0001\n",
      "Epoch: 86, Step: 599, Loss: 121405.4609375, Lr: 0.0001\n",
      "Epoch: 86, Step: 699, Loss: 122999.5, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 87\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15238.4863 (15238.4863) time: 2.1375 data: 2.1026 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15230.6641 (14851.9382) time: 0.2633 data: 0.2341 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2995 s / it)\n",
      "loss 14851.938\n",
      "loss on the 276 test images 14851.94\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 87, Step: 99, Loss: 123186.296875, Lr: 0.0001\n",
      "Epoch: 87, Step: 199, Loss: 122952.9921875, Lr: 0.0001\n",
      "Epoch: 87, Step: 299, Loss: 130618.109375, Lr: 0.0001\n",
      "Epoch: 87, Step: 399, Loss: 123565.1171875, Lr: 0.0001\n",
      "Epoch: 87, Step: 499, Loss: 120063.71875, Lr: 0.0001\n",
      "Epoch: 87, Step: 599, Loss: 124938.671875, Lr: 0.0001\n",
      "Epoch: 87, Step: 699, Loss: 120104.1171875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 88\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15338.9678 (15338.9678) time: 2.1415 data: 2.1077 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15338.9678 (14950.5197) time: 0.2672 data: 0.2361 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3040 s / it)\n",
      "loss 14950.520\n",
      "loss on the 276 test images 14950.52\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 88, Step: 99, Loss: 125675.1015625, Lr: 0.0001\n",
      "Epoch: 88, Step: 199, Loss: 124525.421875, Lr: 0.0001\n",
      "Epoch: 88, Step: 299, Loss: 115277.125, Lr: 0.0001\n",
      "Epoch: 88, Step: 399, Loss: 120588.15625, Lr: 0.0001\n",
      "Epoch: 88, Step: 499, Loss: 121779.75, Lr: 0.0001\n",
      "Epoch: 88, Step: 599, Loss: 122575.3359375, Lr: 0.0001\n",
      "Epoch: 88, Step: 699, Loss: 127011.0390625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 89\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15217.0254 (15217.0254) time: 2.1575 data: 2.1235 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15217.0254 (14854.1772) time: 0.2655 data: 0.2379 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3029 s / it)\n",
      "loss 14854.177\n",
      "loss on the 276 test images 14854.18\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 89, Step: 99, Loss: 121412.90625, Lr: 0.0001\n",
      "Epoch: 89, Step: 199, Loss: 121239.296875, Lr: 0.0001\n",
      "Epoch: 89, Step: 299, Loss: 125123.640625, Lr: 0.0001\n",
      "Epoch: 89, Step: 399, Loss: 121791.8203125, Lr: 0.0001\n",
      "Epoch: 89, Step: 499, Loss: 120257.2734375, Lr: 0.0001\n",
      "Epoch: 89, Step: 599, Loss: 119654.9140625, Lr: 0.0001\n",
      "Epoch: 89, Step: 699, Loss: 116909.84375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 90\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15330.2109 (15330.2109) time: 2.1141 data: 2.0795 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15142.5068 (14811.0330) time: 0.2590 data: 0.2318 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2963 s / it)\n",
      "loss 14811.033\n",
      "loss on the 276 test images 14811.03\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 90, Step: 99, Loss: 124362.03125, Lr: 0.0001\n",
      "Epoch: 90, Step: 199, Loss: 118307.4296875, Lr: 0.0001\n",
      "Epoch: 90, Step: 299, Loss: 124640.828125, Lr: 0.0001\n",
      "Epoch: 90, Step: 399, Loss: 126156.1953125, Lr: 0.0001\n",
      "Epoch: 90, Step: 499, Loss: 124843.828125, Lr: 0.0001\n",
      "Epoch: 90, Step: 599, Loss: 125721.7421875, Lr: 0.0001\n",
      "Epoch: 90, Step: 699, Loss: 125057.6015625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 91\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15489.0537 (15489.0537) time: 2.1527 data: 2.1220 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15256.3623 (14930.4799) time: 0.2671 data: 0.2388 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3076 s / it)\n",
      "loss 14930.480\n",
      "loss on the 276 test images 14930.48\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 91, Step: 99, Loss: 122004.484375, Lr: 0.0001\n",
      "Epoch: 91, Step: 199, Loss: 121481.046875, Lr: 0.0001\n",
      "Epoch: 91, Step: 299, Loss: 125605.40625, Lr: 0.0001\n",
      "Epoch: 91, Step: 399, Loss: 120236.3203125, Lr: 0.0001\n",
      "Epoch: 91, Step: 499, Loss: 120534.21875, Lr: 0.0001\n",
      "Epoch: 91, Step: 599, Loss: 119583.7890625, Lr: 0.0001\n",
      "Epoch: 91, Step: 699, Loss: 128897.5546875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 92\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 15251.4551 (15251.4551) time: 2.2948 data: 2.2609 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15187.1318 (14837.0592) time: 0.2764 data: 0.2535 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3139 s / it)\n",
      "loss 14837.059\n",
      "loss on the 276 test images 14837.06\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 92, Step: 99, Loss: 124127.875, Lr: 0.0001\n",
      "Epoch: 92, Step: 199, Loss: 125531.84375, Lr: 0.0001\n",
      "Epoch: 92, Step: 299, Loss: 127036.140625, Lr: 0.0001\n",
      "Epoch: 92, Step: 399, Loss: 125591.109375, Lr: 0.0001\n",
      "Epoch: 92, Step: 499, Loss: 127263.4140625, Lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92, Step: 599, Loss: 120446.796875, Lr: 0.0001\n",
      "Epoch: 92, Step: 699, Loss: 119810.5, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 93\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:01:01 loss: 15196.6055 (15196.6055) time: 6.7855 data: 6.6453 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15196.6055 (14839.7407) time: 0.8410 data: 0.7927 max mem: 1909\n",
      "Test: Total time: 0:00:07 (0.8780 s / it)\n",
      "loss 14839.741\n",
      "loss on the 276 test images 14839.74\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 93, Step: 99, Loss: 119771.2421875, Lr: 0.0001\n",
      "Epoch: 93, Step: 199, Loss: 124364.140625, Lr: 0.0001\n",
      "Epoch: 93, Step: 299, Loss: 123642.6796875, Lr: 0.0001\n",
      "Epoch: 93, Step: 399, Loss: 124700.09375, Lr: 0.0001\n",
      "Epoch: 93, Step: 499, Loss: 119191.703125, Lr: 0.0001\n",
      "Epoch: 93, Step: 599, Loss: 119071.2734375, Lr: 0.0001\n",
      "Epoch: 93, Step: 699, Loss: 122944.9921875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 94\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15144.6875 (15144.6875) time: 2.1959 data: 2.1612 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15144.6875 (14796.4942) time: 0.2679 data: 0.2404 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3050 s / it)\n",
      "loss 14796.494\n",
      "loss on the 276 test images 14796.49\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 94, Step: 99, Loss: 122340.65625, Lr: 0.0001\n",
      "Epoch: 94, Step: 199, Loss: 121828.3046875, Lr: 0.0001\n",
      "Epoch: 94, Step: 299, Loss: 121652.875, Lr: 0.0001\n",
      "Epoch: 94, Step: 399, Loss: 121528.015625, Lr: 0.0001\n",
      "Epoch: 94, Step: 499, Loss: 123916.40625, Lr: 0.0001\n",
      "Epoch: 94, Step: 599, Loss: 123591.6484375, Lr: 0.0001\n",
      "Epoch: 94, Step: 699, Loss: 121401.9296875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 95\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15441.5557 (15441.5557) time: 2.1818 data: 2.1434 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15441.5557 (15003.9402) time: 0.2633 data: 0.2401 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3017 s / it)\n",
      "loss 15003.940\n",
      "loss on the 276 test images 15003.94\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 95, Step: 99, Loss: 121911.125, Lr: 0.0001\n",
      "Epoch: 95, Step: 199, Loss: 129081.78125, Lr: 0.0001\n",
      "Epoch: 95, Step: 299, Loss: 121958.6484375, Lr: 0.0001\n",
      "Epoch: 95, Step: 399, Loss: 119574.171875, Lr: 0.0001\n",
      "Epoch: 95, Step: 499, Loss: 121945.5625, Lr: 0.0001\n",
      "Epoch: 95, Step: 599, Loss: 124425.9296875, Lr: 0.0001\n",
      "Epoch: 95, Step: 699, Loss: 121856.53125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 96\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15374.9023 (15374.9023) time: 2.1934 data: 2.1580 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15122.1777 (14857.6580) time: 0.2689 data: 0.2410 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3071 s / it)\n",
      "loss 14857.658\n",
      "loss on the 276 test images 14857.66\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 96, Step: 99, Loss: 123161.2890625, Lr: 0.0001\n",
      "Epoch: 96, Step: 199, Loss: 121718.484375, Lr: 0.0001\n",
      "Epoch: 96, Step: 299, Loss: 122408.4375, Lr: 0.0001\n",
      "Epoch: 96, Step: 399, Loss: 119195.28125, Lr: 0.0001\n",
      "Epoch: 96, Step: 499, Loss: 126181.9375, Lr: 0.0001\n",
      "Epoch: 96, Step: 599, Loss: 118690.0625, Lr: 0.0001\n",
      "Epoch: 96, Step: 699, Loss: 126577.6171875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 97\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15228.5488 (15228.5488) time: 2.1254 data: 2.0929 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15142.9258 (14811.8932) time: 0.2667 data: 0.2355 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3031 s / it)\n",
      "loss 14811.893\n",
      "loss on the 276 test images 14811.89\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 97, Step: 99, Loss: 128033.15625, Lr: 0.0001\n",
      "Epoch: 97, Step: 199, Loss: 119190.3046875, Lr: 0.0001\n",
      "Epoch: 97, Step: 299, Loss: 122724.25, Lr: 0.0001\n",
      "Epoch: 97, Step: 399, Loss: 123031.5, Lr: 0.0001\n",
      "Epoch: 97, Step: 499, Loss: 122764.7265625, Lr: 0.0001\n",
      "Epoch: 97, Step: 599, Loss: 123953.0078125, Lr: 0.0001\n",
      "Epoch: 97, Step: 699, Loss: 121784.703125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 98\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:18 loss: 15116.2871 (15116.2871) time: 2.1092 data: 2.0792 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15116.2871 (14784.6131) time: 0.2604 data: 0.2343 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2973 s / it)\n",
      "loss 14784.613\n",
      "loss on the 276 test images 14784.61\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 98, Step: 99, Loss: 119488.2421875, Lr: 0.0001\n",
      "Epoch: 98, Step: 199, Loss: 117849.015625, Lr: 0.0001\n",
      "Epoch: 98, Step: 299, Loss: 122000.3125, Lr: 0.0001\n",
      "Epoch: 98, Step: 399, Loss: 120689.6640625, Lr: 0.0001\n",
      "Epoch: 98, Step: 499, Loss: 121181.953125, Lr: 0.0001\n",
      "Epoch: 98, Step: 599, Loss: 122011.046875, Lr: 0.0001\n",
      "Epoch: 98, Step: 699, Loss: 121289.6796875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 99\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15335.6299 (15335.6299) time: 2.1417 data: 2.1041 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15280.1709 (14895.3303) time: 0.2609 data: 0.2349 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2978 s / it)\n",
      "loss 14895.330\n",
      "loss on the 276 test images 14895.33\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 99, Step: 99, Loss: 129041.8125, Lr: 0.0001\n",
      "Epoch: 99, Step: 199, Loss: 119880.0625, Lr: 0.0001\n",
      "Epoch: 99, Step: 299, Loss: 121051.484375, Lr: 0.0001\n",
      "Epoch: 99, Step: 399, Loss: 121922.1640625, Lr: 0.0001\n",
      "Epoch: 99, Step: 499, Loss: 125492.375, Lr: 0.0001\n",
      "Epoch: 99, Step: 599, Loss: 115926.4296875, Lr: 0.0001\n",
      "Epoch: 99, Step: 699, Loss: 125948.7109375, Lr: 0.0001\n",
      "Saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterable,Optional\n",
    "import math\n",
    "import torch\n",
    "#import torch.multiprocessing\n",
    "#torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import argparse\n",
    "import os\n",
    "#import timm\n",
    "#from timm.utils import accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from util import misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Variational Auto-Encoder Example\")\n",
    "    parser.add_argument('--batch_size',default=32,type=int,help='Batch size per GPU (effective batch size is batch_size*accum_iter* #gpus)')\n",
    "    parser.add_argument('--epochs',default=20,type=int)\n",
    "    parser.add_argument('--accum_iter',default=1,type=int)\n",
    "    #Model parameters\n",
    "    parser.add_argument('--image_size', type=int, default=128 , metavar='N', help='Image size')\n",
    "    parser.add_argument('--z_dim', type=int, default=128, metavar='N', help='the dim of latent variable z(default: 20)')\n",
    "\n",
    "    parser.add_argument('--input_channel', type=int, default=3, metavar='N', help='input channel(default: 1 for MNIST)')\n",
    "\n",
    "    #Optimizer parameters\n",
    "    parser.add_argument('--weight_decay',type=float,default=0.0001)\n",
    "    parser.add_argument('--lr',type=float,default=0.0001,metavar='LR')\n",
    "    parser.add_argument('--root_path',default='D:\\\\jiao\\\\datasets\\\\celeba')\n",
    "    parser.add_argument('--output_dir',default='./output_dir_pretrained',help='path to save,empty for no saving')\n",
    "    parser.add_argument('--log_dir',default='./output_dir_pretrained',help='path to tensorboard log')\n",
    "    \n",
    "    parser.add_argument('--resume',default='',help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch',default=0,type=int,metavar='N')\n",
    "    parser.add_argument('--num_workers',default=5,type=int)\n",
    "    parser.add_argument('--pin_mem',action='store_true')\n",
    "    parser.add_argument('--no_pin_mem',action='store_false',dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "    return parser\n",
    "'''创建预处理的transform'''\n",
    "def build_transform(is_train,args):\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.CenterCrop(168),\n",
    "        torchvision.transforms.Resize((args.image_size,args.image_size)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    " \n",
    "'''创建数据集 返回dataset'''\n",
    "def build_dataset(is_train,args):\n",
    "    transform = build_transform(is_train,args)\n",
    "    path = os.path.join(args.root_path,'train' if is_train else 'test')\n",
    "    dataset = torchvision.datasets.ImageFolder(path,transform= transform)\n",
    "    info = dataset.find_classes(path)\n",
    "    #print(f\"finding classes from {path}: {info[0]}\")\n",
    "    print(f\"mapping classes from {path} to indexes:{info[1]}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def vae_loss(x_hat, x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Calculate the loss. Note that the loss includes two parts.\n",
    "    :param x_hat:\n",
    "    :param x:\n",
    "    :param mu:\n",
    "    :param log_var:\n",
    "    :return: total loss, BCE and KLD of our model\n",
    "    \"\"\"\n",
    "    # 1. the reconstruction loss. 重建损失\n",
    "    # We regard the MNIST as binary classification\n",
    "    #BCE = F.binary_cross_entropy(x_hat, x, reduction='sum')#MINST等二值图像可以用交叉熵\n",
    "    #jtq20240214 非二值图像用均方误差\n",
    "    BCE = F.mse_loss(x_hat , x , reduction='sum')\n",
    "\n",
    "    # 2. KL-divergence KL散度损失\n",
    "    # D_KL(Q(z|X) || P(z)); calculate in closed form as both dist. are Gaussian\n",
    "    # here we assume that \\Sigma is a diagonal matrix, so as to simplify the computation\n",
    "    # D_KL(Q(z|X) || N(0,1)) = 0.5*( -1 - log(sigma^2) + mu^2 + sigma^2)\n",
    "    # log_var = log(sigma^2)\n",
    "    KLD = 0.5 * torch.sum(torch.exp(log_var) + torch.pow(mu, 2) - 1. - log_var)\n",
    "\n",
    "    # 3. total loss 总损失 = 重建损失 + KL散度损失\n",
    "    loss = BCE + KLD \n",
    "    return loss, BCE, KLD\n",
    "''' 验证函数\n",
    "    输入：\n",
    "    输出：\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def evaluate(data_loader,model,device,epoch):\n",
    "    criterion = vae_loss\n",
    "    metric_logger = misc.MetricLogger(delimiter=\" \")\n",
    "    header = 'Test:'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    #test_avg_loss = 0.0\n",
    "    #下面这段话基本等价于 for (images, targets) in data_loader：\n",
    "    for batch in metric_logger.log_every(data_loader,\n",
    "                                         100, #打印间隔\n",
    "                                         header): #标题\n",
    "        images = batch[0]\n",
    "        target = batch[-1]\n",
    "        images = images.to(device,non_blocking=True)\n",
    "        target = target.to(device,non_blocking=True)\n",
    "        # 前向传播\n",
    "        test_x_hat, test_mu, test_log_var = model(images)\n",
    "        \n",
    "        # 损失函数值\n",
    "        test_loss, test_BCE, test_KLD = criterion(test_x_hat, images, test_mu, test_log_var)\n",
    "        #test_avg_loss += test_loss\n",
    "        batch_size = images.shape[0]\n",
    "        metric_logger.update(loss = test_loss.item())\n",
    "        \n",
    "    # 对和求平均，得到每一张图片的平均损失\n",
    "    #test_avg_loss /= len(mnist_test.dataset)    \n",
    "    '''测试随机生成的隐变量'''\n",
    "    # 随机从隐变量的分布中取隐变量\n",
    "    z = torch.randn(32, args.z_dim).to(device)  # 每一行是一个隐变量，总共有batch_size行\n",
    "    z = model.decoder_projection(z)\n",
    "    # reshape\n",
    "    z = torch.reshape(z,(-1, *model.decoder_in_chw))\n",
    "    \n",
    "    # 对隐变量重构\n",
    "    random_res = model.decode(z)\n",
    "    # 保存重构结果\n",
    "    save_image(random_res, f\"{args.output_dir}/random_sampled-{epoch}.png\" )\n",
    "\n",
    "    \n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('loss {losses.global_avg:.3f}'.format(losses=metric_logger.loss))\n",
    "    return {k:meter.global_avg for k,meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "'''\n",
    "    训练函数\n",
    "'''\n",
    "def train_one_epoch(model:torch.nn.Module,criterion:torch.nn.Module,\n",
    "                    data_loader:Iterable,optimizer:torch.optim.Optimizer,\n",
    "                    device:torch.device,epoch:int,loss_scaler,max_norm: float=0,\n",
    "                    log_writer=None,args=None):\n",
    "    model.train(True)\n",
    "    print_freq = 2\n",
    "    accum_iter = args.accum_iter\n",
    "    #print(\"in train_one_epoch\")\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "    for data_iter_step,(samples,targets) in enumerate(data_loader):\n",
    "        samples = samples.to(device,non_blocking=True)\n",
    "        targets = targets.to(device,non_blocking=True)\n",
    "        \n",
    "        #print(\"input_dim:\",args.input_dim)\n",
    "        #print(\"samples shape:\",samples.shape)\n",
    "        x_hat, mu, log_var = model(samples)\n",
    "        warmup_lr = args.lr\n",
    "        optimizer.param_groups[0][\"lr\"] = warmup_lr\n",
    "        \n",
    "        loss , _ , _ = criterion(x_hat,samples, mu,log_var)\n",
    "        loss /= accum_iter\n",
    "        \n",
    "        loss_scaler(loss,optimizer,clip_grad=max_norm, \n",
    "                    parameters=model.parameters(),create_graph=False,\n",
    "                    update_grad=(data_iter_step+1)%accum_iter == 0) #训练每accum_iter个batch才更新梯度\n",
    "        loss_value = loss.item()\n",
    "        if (data_iter_step+1)%accum_iter == 0:\n",
    "            optimizer.zero_grad()\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"loss is {loss_value}, stopping training\")\n",
    "            sys.exit(1)\n",
    "        if log_writer is not None and (data_iter_step+1)% (accum_iter*100) == 0 :\n",
    "            epoch_1000x = int((data_iter_step/len(data_loader)+epoch)*1000)\n",
    "            log_writer.add_scalar('loss',loss_value,epoch_1000x)\n",
    "            log_writer.add_scalar('lr',warmup_lr,epoch_1000x)\n",
    "            print(f\"Epoch: {epoch}, Step: {data_iter_step}, Loss: {loss}, Lr: {warmup_lr}\")\n",
    "\n",
    "\n",
    "def main(args,mode='train',test_image_path=''):\n",
    "    print(f\"当前mode: {mode}\")\n",
    "    if mode =='train':\n",
    "        #构建批次\n",
    "        dataset_train = build_dataset(is_train=True,args=args)\n",
    "        dataset_val = build_dataset(is_train=False,args=args)\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "        data_loader_train = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_train,sampler=sampler_train,\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=args.pin_mem,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_val,sampler=sampler_val,\n",
    "            batch_size=32,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=args.pin_mem,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        \n",
    "        #构建模型\n",
    "        model = VAE(z_dim = args.z_dim,image_size=args.image_size,ch=args.input_channel)\n",
    "        #model = VAE(args.input_dim,args.hid_dim,args.z_dim)\n",
    "        model = model.to(device)\n",
    "        n_parameters = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "        print(f\"number of trainable parameters(M):{n_parameters/1.e6:.2f}\") #f-string保留两位小数{xxx:.2f}\n",
    "        criterion = vae_loss\n",
    "        \n",
    "        #weight_decay就是对损失函数做L2正则化，防止过拟合\n",
    "        optimizer = torch.optim.AdamW(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)     \n",
    "        #用tensorboard记录日志\n",
    "        os.makedirs(args.log_dir,exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "        #lossScaler用来反传梯度用的\n",
    "        loss_scaler = NativeScaler()\n",
    "        \n",
    "        #读入已有的模型 resume为空字符串 则不会读取，如果传入时pth文件，则会读取原来的模型\n",
    "        #读取进来时args里epoch会+1\n",
    "        misc.load_model(args=args,model_without_ddp=model,optimizer=optimizer,loss_scaler=loss_scaler)\n",
    "        for epoch in range(args.start_epoch,args.epochs): #start_epoch开始训练\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            print(f\"length of data_loader_train is {len(data_loader_train)}\") #几个batch\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Evaluating...\")\n",
    "                model.eval()\n",
    "                test_stats = evaluate(data_loader_val,model,device,epoch)\n",
    "                print(f\"loss on the {len(dataset_val)} test images {test_stats['loss']:.2f}\")\n",
    "                if log_writer is not None:\n",
    "                    ''' \n",
    "                        add_scalar(tag, scalar_value, global_step=None, walltime=None) \n",
    "                        add_scalar:记录标量函数,参数:\n",
    "                        tag：图的名称 scalar_value：记录的值 global_step：x轴\n",
    "                    '''\n",
    "                    log_writer.add_scalar('perf/test_loss',test_stats['loss'],epoch)\n",
    "                model.train()    \n",
    "            print(\"Training...\")\n",
    "            train_stats = train_one_epoch(\n",
    "                model,criterion,data_loader_train,\n",
    "                optimizer,device,epoch,#epoch+1, #为什么要+1？\n",
    "                loss_scaler,None,\n",
    "                log_writer=log_writer,args=args\n",
    "            )\n",
    "            if args.output_dir:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                misc.save_model(args=args,model=model,model_without_ddp=model,optimizer=optimizer,\n",
    "                               loss_scaler=loss_scaler,epoch=epoch)\n",
    "            #break\n",
    "        \n",
    "           \n",
    "'''main'''      \n",
    "if __name__ == '__main__':\n",
    "#     z_dim = 64 #隐空间维度\n",
    "#     hid_dim = 512 #encoder和decoder中间层的维度\n",
    "#     in_dim = 128 * 128 \n",
    "#     #out_dim = 28*28 #图片维度\n",
    "#     ch = 3\n",
    "#     bs =128\n",
    "#     x = torch.randn(bs,ch,128,128)\n",
    "#     model = VAE(z_dim = z_dim,image_size=128,ch=3)\n",
    "#     re_x, mu, log_var = model(x)\n",
    "#     print(re_x.shape)\n",
    "    args = get_args_parser()\n",
    "    args = args.parse_args(args=['--batch_size','256','--epochs','100','--num_workers','2','--resume','./output_dir_pretrained/checkpoint-29.pth'])\n",
    "#     dataset = build_dataset(is_train=True,args=args)\n",
    "#     print(dataset[1])\n",
    "    main(args = args,mode='train')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a776e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 32, 128, 256, 3]\n",
      "[16, 32, 128, 256, 3]\n"
     ]
    }
   ],
   "source": [
    "hiddens=[16,32,128,256]\n",
    "print(hiddens)\n",
    "\n",
    "print( hiddens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
