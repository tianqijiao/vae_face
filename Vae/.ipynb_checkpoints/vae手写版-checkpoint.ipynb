{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a4467364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "'''\n",
    "VAE模型\n",
    "\n",
    "'''\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hiddens=[16,32,128,256], z_dim=128,image_size=128,ch=3):\n",
    "        # 调用父类方法初始化模块的state\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        prev_ch = ch\n",
    "        modules = []\n",
    "        cur_image_size = image_size\n",
    "        # 编码器 ： [bs,ch, input_dim] => [bs,ch, z_dim]\n",
    "        for cur_ch in hiddens:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(prev_ch,cur_ch,kernel_size=3,stride=2,padding=1), #stride=2 图片每次缩小一半\n",
    "                    nn.BatchNorm2d(cur_ch),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            '''通道数每次卷积X2，图片大小每次 /2\n",
    "                类似UNet的操作\n",
    "            '''\n",
    "            prev_ch = cur_ch\n",
    "            cur_image_size //= 2\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.mean_linear = nn.Linear(prev_ch * cur_image_size * cur_image_size,z_dim)\n",
    "        self.var_linear = nn.Linear(prev_ch * cur_image_size * cur_image_size,z_dim)\n",
    "\n",
    "        # 解码器 ： [bs,ch, z_dim] => [bs,ch, input_dim]\n",
    "        modules = []\n",
    "        #prev_ch = 256\n",
    "        self.decoder_projection = nn.Linear(z_dim,prev_ch * cur_image_size * cur_image_size)\n",
    "        self.decoder_in_chw = (prev_ch, cur_image_size, cur_image_size)\n",
    "        for i , cur_ch in enumerate( hiddens[::-1] ):\n",
    "            if i == 0:\n",
    "                pass\n",
    "            else:\n",
    "                modules.append(\n",
    "                    nn.Sequential(\n",
    "                        nn.ConvTranspose2d(prev_ch,cur_ch,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                        nn.BatchNorm2d(cur_ch),\n",
    "                        nn.ReLU()\n",
    "                    )\n",
    "                )\n",
    "            prev_ch = cur_ch\n",
    "        #(1,256,8,8) 做完反卷积(3次反卷积 分别是256to128 128to32 32to16 ) 变成(1,16,64,64)\n",
    "        modules.append(\n",
    "            nn.Sequential(\n",
    "                #先用反卷积把(1,16,64,64) 变成(1，16,128,128)\n",
    "                #再卷积成三通道\n",
    "                nn.ConvTranspose2d(prev_ch,prev_ch,kernel_size=3,stride=2,padding=1,output_padding=1),\n",
    "                nn.BatchNorm2d(cur_ch),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(cur_ch,3,kernel_size=3,stride=1,padding=1),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        向前传播部分, 在model_name(inputs)时自动调用\n",
    "        \"\"\"\n",
    "        # encoder\n",
    "        mu, log_var = self.encode(x)\n",
    "        \n",
    "        # reparameterization trick\n",
    "        sampled_z = self.reparameterization(mu, log_var)\n",
    "        sampled_z = self.decoder_projection(sampled_z)\n",
    "        # reshape\n",
    "        sampled_z = torch.reshape(sampled_z,(-1, *self.decoder_in_chw))\n",
    "        # decoder\n",
    "        #print(sampled_z.shape)\n",
    "        x_hat = self.decode(sampled_z)\n",
    "        #print(x_hat.shape)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        encoding part\n",
    "        :param x: input image\n",
    "        :return: mu and log_var\n",
    "        \"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = torch.flatten(x,1) #把 (bs,256,h, w) 压平成 (bs, ...)\n",
    "        mu = self.mean_linear(x)\n",
    "        log_var = self.var_linear(x)\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "    def reparameterization(self, mu, log_var): #重参数化采样z\n",
    "        \"\"\"\n",
    "        Given a standard gaussian distribution epsilon ~ N(0,1),\n",
    "        we can sample the random variable z as per z = mu + sigma * epsilon\n",
    "        :param mu:\n",
    "        :param log_var:\n",
    "        :return: sampled z\n",
    "        \"\"\"\n",
    "        sigma = torch.exp(log_var * 0.5) #标准差sigma, 方差的log log_var\n",
    "        eps = torch.randn_like(sigma)\n",
    "        return mu + sigma * eps  # 这里的“*”是点乘的意思\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Given a sampled z, decode it back to image\n",
    "        :param z:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x_hat = self.decoder(z)\n",
    "        #x_hat = torch.sigmoid(self.fc5(h))  # 图片数值取值为[0,1]，不宜用ReLU\n",
    "        return x_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b52c7016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前mode: train\n",
      "mapping classes from D:\\jiao\\datasets\\celeba\\train to indexes:{'face': 0}\n",
      "mapping classes from D:\\jiao\\datasets\\celeba\\test to indexes:{'face': 0}\n",
      "number of trainable parameters(M):6.99\n",
      "Resume checkpoint ./output_dir_pretrained/checkpoint-4.pth\n",
      "With optim & sched!\n",
      "Epoch 5\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:31 loss: 18838.9277 (18838.9277) time: 3.5117 data: 3.3702 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 18838.9277 (18438.4068) time: 0.4743 data: 0.4389 max mem: 1909\n",
      "Test: Total time: 0:00:04 (0.5179 s / it)\n",
      "loss 18438.407\n",
      "loss on the 276 test images 18438.41\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 5, Step: 99, Loss: 155902.671875, Lr: 0.0001\n",
      "Epoch: 5, Step: 199, Loss: 160758.15625, Lr: 0.0001\n",
      "Epoch: 5, Step: 299, Loss: 155493.25, Lr: 0.0001\n",
      "Epoch: 5, Step: 399, Loss: 151976.40625, Lr: 0.0001\n",
      "Epoch: 5, Step: 499, Loss: 149445.890625, Lr: 0.0001\n",
      "Epoch: 5, Step: 599, Loss: 150650.953125, Lr: 0.0001\n",
      "Epoch: 5, Step: 699, Loss: 158824.0, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 6\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:34 loss: 18432.8477 (18432.8477) time: 3.8319 data: 3.6844 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 18588.0664 (18096.2528) time: 0.5459 data: 0.5132 max mem: 1909\n",
      "Test: Total time: 0:00:05 (0.5894 s / it)\n",
      "loss 18096.253\n",
      "loss on the 276 test images 18096.25\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 6, Step: 99, Loss: 153832.828125, Lr: 0.0001\n",
      "Epoch: 6, Step: 199, Loss: 152778.75, Lr: 0.0001\n",
      "Epoch: 6, Step: 299, Loss: 147188.875, Lr: 0.0001\n",
      "Epoch: 6, Step: 399, Loss: 153688.453125, Lr: 0.0001\n",
      "Epoch: 6, Step: 499, Loss: 145590.25, Lr: 0.0001\n",
      "Epoch: 6, Step: 599, Loss: 153045.765625, Lr: 0.0001\n",
      "Epoch: 6, Step: 699, Loss: 145578.84375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 7\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:23 loss: 17785.0508 (17785.0508) time: 2.6145 data: 2.5696 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 17874.7227 (17537.7075) time: 0.3216 data: 0.2868 max mem: 1909\n",
      "Test: Total time: 0:00:03 (0.3606 s / it)\n",
      "loss 17537.707\n",
      "loss on the 276 test images 17537.71\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 7, Step: 99, Loss: 143786.984375, Lr: 0.0001\n",
      "Epoch: 7, Step: 199, Loss: 144329.5625, Lr: 0.0001\n",
      "Epoch: 7, Step: 299, Loss: 145751.09375, Lr: 0.0001\n",
      "Epoch: 7, Step: 399, Loss: 138780.21875, Lr: 0.0001\n",
      "Epoch: 7, Step: 499, Loss: 151169.25, Lr: 0.0001\n",
      "Epoch: 7, Step: 599, Loss: 141554.96875, Lr: 0.0001\n",
      "Epoch: 7, Step: 699, Loss: 146130.703125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 8\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 17631.2871 (17631.2871) time: 2.1613 data: 2.1231 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 17631.2871 (17287.6228) time: 0.2623 data: 0.2371 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2993 s / it)\n",
      "loss 17287.623\n",
      "loss on the 276 test images 17287.62\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 8, Step: 99, Loss: 145617.96875, Lr: 0.0001\n",
      "Epoch: 8, Step: 199, Loss: 142576.25, Lr: 0.0001\n",
      "Epoch: 8, Step: 299, Loss: 136735.109375, Lr: 0.0001\n",
      "Epoch: 8, Step: 399, Loss: 147926.890625, Lr: 0.0001\n",
      "Epoch: 8, Step: 499, Loss: 145724.796875, Lr: 0.0001\n",
      "Epoch: 8, Step: 599, Loss: 146714.5625, Lr: 0.0001\n",
      "Epoch: 8, Step: 699, Loss: 146360.71875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 9\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 17455.4434 (17455.4434) time: 2.1795 data: 2.1426 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 17455.4434 (17050.7565) time: 0.2654 data: 0.2391 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3024 s / it)\n",
      "loss 17050.757\n",
      "loss on the 276 test images 17050.76\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 9, Step: 99, Loss: 142558.984375, Lr: 0.0001\n",
      "Epoch: 9, Step: 199, Loss: 142440.609375, Lr: 0.0001\n",
      "Epoch: 9, Step: 299, Loss: 139288.5, Lr: 0.0001\n",
      "Epoch: 9, Step: 399, Loss: 138104.28125, Lr: 0.0001\n",
      "Epoch: 9, Step: 499, Loss: 144087.1875, Lr: 0.0001\n",
      "Epoch: 9, Step: 599, Loss: 144043.671875, Lr: 0.0001\n",
      "Epoch: 9, Step: 699, Loss: 146298.6875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 10\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 17267.7656 (17267.7656) time: 2.1302 data: 2.0964 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 17270.8496 (16928.6469) time: 0.2658 data: 0.2356 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3036 s / it)\n",
      "loss 16928.647\n",
      "loss on the 276 test images 16928.65\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 10, Step: 99, Loss: 140894.21875, Lr: 0.0001\n",
      "Epoch: 10, Step: 199, Loss: 143375.984375, Lr: 0.0001\n",
      "Epoch: 10, Step: 299, Loss: 137216.5, Lr: 0.0001\n",
      "Epoch: 10, Step: 399, Loss: 136880.25, Lr: 0.0001\n",
      "Epoch: 10, Step: 499, Loss: 140972.609375, Lr: 0.0001\n",
      "Epoch: 10, Step: 599, Loss: 140234.390625, Lr: 0.0001\n",
      "Epoch: 10, Step: 699, Loss: 138131.78125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 11\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 17161.9551 (17161.9551) time: 2.1449 data: 2.1058 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 17161.9551 (16758.7192) time: 0.2678 data: 0.2372 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3072 s / it)\n",
      "loss 16758.719\n",
      "loss on the 276 test images 16758.72\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 11, Step: 99, Loss: 134582.75, Lr: 0.0001\n",
      "Epoch: 11, Step: 199, Loss: 137431.75, Lr: 0.0001\n",
      "Epoch: 11, Step: 299, Loss: 142169.71875, Lr: 0.0001\n",
      "Epoch: 11, Step: 399, Loss: 138035.28125, Lr: 0.0001\n",
      "Epoch: 11, Step: 499, Loss: 137537.71875, Lr: 0.0001\n",
      "Epoch: 11, Step: 599, Loss: 142444.3125, Lr: 0.0001\n",
      "Epoch: 11, Step: 699, Loss: 138732.59375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 12\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:20 loss: 17007.0859 (17007.0859) time: 2.2232 data: 2.1855 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 17104.3711 (16654.6888) time: 0.2699 data: 0.2443 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3078 s / it)\n",
      "loss 16654.689\n",
      "loss on the 276 test images 16654.69\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 12, Step: 99, Loss: 138451.734375, Lr: 0.0001\n",
      "Epoch: 12, Step: 199, Loss: 140135.4375, Lr: 0.0001\n",
      "Epoch: 12, Step: 299, Loss: 141199.375, Lr: 0.0001\n",
      "Epoch: 12, Step: 399, Loss: 140207.875, Lr: 0.0001\n",
      "Epoch: 12, Step: 499, Loss: 138955.96875, Lr: 0.0001\n",
      "Epoch: 12, Step: 599, Loss: 138199.3125, Lr: 0.0001\n",
      "Epoch: 12, Step: 699, Loss: 139645.0625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 13\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16734.3066 (16734.3066) time: 2.1673 data: 2.1297 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16896.2070 (16533.6573) time: 0.2662 data: 0.2380 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3036 s / it)\n",
      "loss 16533.657\n",
      "loss on the 276 test images 16533.66\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 13, Step: 99, Loss: 140147.828125, Lr: 0.0001\n",
      "Epoch: 13, Step: 199, Loss: 146483.390625, Lr: 0.0001\n",
      "Epoch: 13, Step: 299, Loss: 135757.96875, Lr: 0.0001\n",
      "Epoch: 13, Step: 399, Loss: 138277.59375, Lr: 0.0001\n",
      "Epoch: 13, Step: 499, Loss: 135995.59375, Lr: 0.0001\n",
      "Epoch: 13, Step: 599, Loss: 144410.8125, Lr: 0.0001\n",
      "Epoch: 13, Step: 699, Loss: 136573.125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 14\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16808.9570 (16808.9570) time: 2.1643 data: 2.1296 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16835.3789 (16539.6317) time: 0.2671 data: 0.2390 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3035 s / it)\n",
      "loss 16539.632\n",
      "loss on the 276 test images 16539.63\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 14, Step: 99, Loss: 134402.3125, Lr: 0.0001\n",
      "Epoch: 14, Step: 199, Loss: 138008.84375, Lr: 0.0001\n",
      "Epoch: 14, Step: 299, Loss: 134859.015625, Lr: 0.0001\n",
      "Epoch: 14, Step: 399, Loss: 134583.828125, Lr: 0.0001\n",
      "Epoch: 14, Step: 499, Loss: 141559.9375, Lr: 0.0001\n",
      "Epoch: 14, Step: 599, Loss: 133742.25, Lr: 0.0001\n",
      "Epoch: 14, Step: 699, Loss: 134257.6875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 15\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16647.1289 (16647.1289) time: 2.1604 data: 2.1277 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16647.1289 (16355.8613) time: 0.2691 data: 0.2374 max mem: 1909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Total time: 0:00:02 (0.3061 s / it)\n",
      "loss 16355.861\n",
      "loss on the 276 test images 16355.86\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 15, Step: 99, Loss: 139127.09375, Lr: 0.0001\n",
      "Epoch: 15, Step: 199, Loss: 139026.5, Lr: 0.0001\n",
      "Epoch: 15, Step: 299, Loss: 138588.5, Lr: 0.0001\n",
      "Epoch: 15, Step: 399, Loss: 132955.125, Lr: 0.0001\n",
      "Epoch: 15, Step: 499, Loss: 132687.15625, Lr: 0.0001\n",
      "Epoch: 15, Step: 599, Loss: 133745.734375, Lr: 0.0001\n",
      "Epoch: 15, Step: 699, Loss: 141462.375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 16\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16625.9492 (16625.9492) time: 2.1382 data: 2.1053 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16637.4023 (16308.3953) time: 0.2598 data: 0.2367 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2977 s / it)\n",
      "loss 16308.395\n",
      "loss on the 276 test images 16308.40\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 16, Step: 99, Loss: 137840.875, Lr: 0.0001\n",
      "Epoch: 16, Step: 199, Loss: 134635.0, Lr: 0.0001\n",
      "Epoch: 16, Step: 299, Loss: 137214.34375, Lr: 0.0001\n",
      "Epoch: 16, Step: 399, Loss: 133070.0, Lr: 0.0001\n",
      "Epoch: 16, Step: 499, Loss: 133673.4375, Lr: 0.0001\n",
      "Epoch: 16, Step: 599, Loss: 141376.09375, Lr: 0.0001\n",
      "Epoch: 16, Step: 699, Loss: 134402.453125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 17\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:18 loss: 16469.0664 (16469.0664) time: 2.1108 data: 2.0760 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16637.1582 (16189.0348) time: 0.2669 data: 0.2378 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3038 s / it)\n",
      "loss 16189.035\n",
      "loss on the 276 test images 16189.03\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 17, Step: 99, Loss: 130435.28125, Lr: 0.0001\n",
      "Epoch: 17, Step: 199, Loss: 137146.90625, Lr: 0.0001\n",
      "Epoch: 17, Step: 299, Loss: 139457.15625, Lr: 0.0001\n",
      "Epoch: 17, Step: 399, Loss: 136054.734375, Lr: 0.0001\n",
      "Epoch: 17, Step: 499, Loss: 137593.875, Lr: 0.0001\n",
      "Epoch: 17, Step: 599, Loss: 132038.6875, Lr: 0.0001\n",
      "Epoch: 17, Step: 699, Loss: 136322.015625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 18\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16295.7520 (16295.7520) time: 2.1610 data: 2.1243 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16591.6875 (16064.7279) time: 0.2658 data: 0.2377 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3034 s / it)\n",
      "loss 16064.728\n",
      "loss on the 276 test images 16064.73\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 18, Step: 99, Loss: 138103.40625, Lr: 0.0001\n",
      "Epoch: 18, Step: 199, Loss: 132853.5625, Lr: 0.0001\n",
      "Epoch: 18, Step: 299, Loss: 135787.40625, Lr: 0.0001\n",
      "Epoch: 18, Step: 399, Loss: 129146.9375, Lr: 0.0001\n",
      "Epoch: 18, Step: 499, Loss: 132068.5625, Lr: 0.0001\n",
      "Epoch: 18, Step: 599, Loss: 133678.546875, Lr: 0.0001\n",
      "Epoch: 18, Step: 699, Loss: 132529.40625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 19\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16359.7656 (16359.7656) time: 2.1371 data: 2.0995 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16359.7656 (16036.2055) time: 0.2590 data: 0.2347 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2952 s / it)\n",
      "loss 16036.206\n",
      "loss on the 276 test images 16036.21\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 19, Step: 99, Loss: 133843.9375, Lr: 0.0001\n",
      "Epoch: 19, Step: 199, Loss: 135776.78125, Lr: 0.0001\n",
      "Epoch: 19, Step: 299, Loss: 132433.015625, Lr: 0.0001\n",
      "Epoch: 19, Step: 399, Loss: 135344.90625, Lr: 0.0001\n",
      "Epoch: 19, Step: 499, Loss: 127261.265625, Lr: 0.0001\n",
      "Epoch: 19, Step: 599, Loss: 129227.875, Lr: 0.0001\n",
      "Epoch: 19, Step: 699, Loss: 134666.171875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 20\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16375.9814 (16375.9814) time: 2.1909 data: 2.1595 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16375.9814 (15959.5126) time: 0.2696 data: 0.2414 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3077 s / it)\n",
      "loss 15959.513\n",
      "loss on the 276 test images 15959.51\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 20, Step: 99, Loss: 139071.5625, Lr: 0.0001\n",
      "Epoch: 20, Step: 199, Loss: 131032.7578125, Lr: 0.0001\n",
      "Epoch: 20, Step: 299, Loss: 133641.15625, Lr: 0.0001\n",
      "Epoch: 20, Step: 399, Loss: 131644.125, Lr: 0.0001\n",
      "Epoch: 20, Step: 499, Loss: 130981.0625, Lr: 0.0001\n",
      "Epoch: 20, Step: 599, Loss: 130938.4296875, Lr: 0.0001\n",
      "Epoch: 20, Step: 699, Loss: 130496.328125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 21\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16364.5527 (16364.5527) time: 2.1277 data: 2.0914 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16364.5527 (15979.9360) time: 0.2599 data: 0.2337 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2967 s / it)\n",
      "loss 15979.936\n",
      "loss on the 276 test images 15979.94\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 21, Step: 99, Loss: 133679.40625, Lr: 0.0001\n",
      "Epoch: 21, Step: 199, Loss: 135406.75, Lr: 0.0001\n",
      "Epoch: 21, Step: 299, Loss: 128777.234375, Lr: 0.0001\n",
      "Epoch: 21, Step: 399, Loss: 130544.4296875, Lr: 0.0001\n",
      "Epoch: 21, Step: 499, Loss: 131119.625, Lr: 0.0001\n",
      "Epoch: 21, Step: 599, Loss: 132962.5625, Lr: 0.0001\n",
      "Epoch: 21, Step: 699, Loss: 130371.390625, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 22\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16239.0732 (16239.0732) time: 2.1452 data: 2.1145 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16239.0732 (15891.7560) time: 0.2687 data: 0.2391 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3054 s / it)\n",
      "loss 15891.756\n",
      "loss on the 276 test images 15891.76\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 22, Step: 99, Loss: 126767.21875, Lr: 0.0001\n",
      "Epoch: 22, Step: 199, Loss: 135780.8125, Lr: 0.0001\n",
      "Epoch: 22, Step: 299, Loss: 129098.21875, Lr: 0.0001\n",
      "Epoch: 22, Step: 399, Loss: 128176.3203125, Lr: 0.0001\n",
      "Epoch: 22, Step: 499, Loss: 132845.28125, Lr: 0.0001\n",
      "Epoch: 22, Step: 599, Loss: 133810.71875, Lr: 0.0001\n",
      "Epoch: 22, Step: 699, Loss: 131609.421875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 23\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15877.0771 (15877.0771) time: 2.1480 data: 2.1135 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16127.6191 (15712.7411) time: 0.2641 data: 0.2359 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3027 s / it)\n",
      "loss 15712.741\n",
      "loss on the 276 test images 15712.74\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 23, Step: 99, Loss: 132101.015625, Lr: 0.0001\n",
      "Epoch: 23, Step: 199, Loss: 134832.03125, Lr: 0.0001\n",
      "Epoch: 23, Step: 299, Loss: 130008.1796875, Lr: 0.0001\n",
      "Epoch: 23, Step: 399, Loss: 135725.625, Lr: 0.0001\n",
      "Epoch: 23, Step: 499, Loss: 132354.359375, Lr: 0.0001\n",
      "Epoch: 23, Step: 599, Loss: 133488.71875, Lr: 0.0001\n",
      "Epoch: 23, Step: 699, Loss: 129291.0, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 24\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16177.8096 (16177.8096) time: 2.1776 data: 2.1421 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16038.3887 (15699.9067) time: 0.2664 data: 0.2395 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3019 s / it)\n",
      "loss 15699.907\n",
      "loss on the 276 test images 15699.91\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 24, Step: 99, Loss: 127287.9453125, Lr: 0.0001\n",
      "Epoch: 24, Step: 199, Loss: 129937.2421875, Lr: 0.0001\n",
      "Epoch: 24, Step: 299, Loss: 132159.0625, Lr: 0.0001\n",
      "Epoch: 24, Step: 399, Loss: 139534.828125, Lr: 0.0001\n",
      "Epoch: 24, Step: 499, Loss: 138706.171875, Lr: 0.0001\n",
      "Epoch: 24, Step: 599, Loss: 135942.578125, Lr: 0.0001\n",
      "Epoch: 24, Step: 699, Loss: 130980.359375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 25\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16191.6758 (16191.6758) time: 2.1428 data: 2.1061 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16175.8682 (15697.2766) time: 0.2613 data: 0.2361 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2993 s / it)\n",
      "loss 15697.277\n",
      "loss on the 276 test images 15697.28\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 25, Step: 99, Loss: 130994.7421875, Lr: 0.0001\n",
      "Epoch: 25, Step: 199, Loss: 125586.375, Lr: 0.0001\n",
      "Epoch: 25, Step: 299, Loss: 126845.625, Lr: 0.0001\n",
      "Epoch: 25, Step: 399, Loss: 130801.96875, Lr: 0.0001\n",
      "Epoch: 25, Step: 499, Loss: 129311.90625, Lr: 0.0001\n",
      "Epoch: 25, Step: 599, Loss: 131153.3125, Lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Step: 699, Loss: 130889.453125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 26\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16070.5625 (16070.5625) time: 2.1938 data: 2.1578 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16070.5625 (15658.8483) time: 0.2691 data: 0.2410 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3063 s / it)\n",
      "loss 15658.848\n",
      "loss on the 276 test images 15658.85\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 26, Step: 99, Loss: 130021.390625, Lr: 0.0001\n",
      "Epoch: 26, Step: 199, Loss: 128552.21875, Lr: 0.0001\n",
      "Epoch: 26, Step: 299, Loss: 133231.40625, Lr: 0.0001\n",
      "Epoch: 26, Step: 399, Loss: 127433.6328125, Lr: 0.0001\n",
      "Epoch: 26, Step: 499, Loss: 130199.84375, Lr: 0.0001\n",
      "Epoch: 26, Step: 599, Loss: 128951.0078125, Lr: 0.0001\n",
      "Epoch: 26, Step: 699, Loss: 126919.84375, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 27\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16013.4531 (16013.4531) time: 2.1533 data: 2.1156 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16013.4531 (15606.5481) time: 0.2615 data: 0.2368 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.2995 s / it)\n",
      "loss 15606.548\n",
      "loss on the 276 test images 15606.55\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 27, Step: 99, Loss: 128260.6015625, Lr: 0.0001\n",
      "Epoch: 27, Step: 199, Loss: 128472.53125, Lr: 0.0001\n",
      "Epoch: 27, Step: 299, Loss: 128495.875, Lr: 0.0001\n",
      "Epoch: 27, Step: 399, Loss: 128470.7265625, Lr: 0.0001\n",
      "Epoch: 27, Step: 499, Loss: 131311.765625, Lr: 0.0001\n",
      "Epoch: 27, Step: 599, Loss: 129916.71875, Lr: 0.0001\n",
      "Epoch: 27, Step: 699, Loss: 124988.046875, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 28\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 16054.2070 (16054.2070) time: 2.1547 data: 2.1203 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 16048.3623 (15642.2934) time: 0.2659 data: 0.2386 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3033 s / it)\n",
      "loss 15642.293\n",
      "loss on the 276 test images 15642.29\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 28, Step: 99, Loss: 131173.5625, Lr: 0.0001\n",
      "Epoch: 28, Step: 199, Loss: 132969.0625, Lr: 0.0001\n",
      "Epoch: 28, Step: 299, Loss: 128100.9140625, Lr: 0.0001\n",
      "Epoch: 28, Step: 399, Loss: 130057.859375, Lr: 0.0001\n",
      "Epoch: 28, Step: 499, Loss: 132874.25, Lr: 0.0001\n",
      "Epoch: 28, Step: 599, Loss: 130262.0625, Lr: 0.0001\n",
      "Epoch: 28, Step: 699, Loss: 126982.3828125, Lr: 0.0001\n",
      "Saving checkpoint...\n",
      "Epoch 29\n",
      "length of data_loader_train is 790\n",
      "Evaluating...\n",
      "Test: [0/9] eta: 0:00:19 loss: 15886.0762 (15886.0762) time: 2.1168 data: 2.0828 max mem: 1909\n",
      "Test: [8/9] eta: 0:00:00 loss: 15944.5762 (15537.4843) time: 0.2679 data: 0.2389 max mem: 1909\n",
      "Test: Total time: 0:00:02 (0.3070 s / it)\n",
      "loss 15537.484\n",
      "loss on the 276 test images 15537.48\n",
      "Training...\n",
      "log_dir: ./output_dir_pretrained\n",
      "Epoch: 29, Step: 99, Loss: 127043.0078125, Lr: 0.0001\n",
      "Epoch: 29, Step: 199, Loss: 130341.0625, Lr: 0.0001\n",
      "Epoch: 29, Step: 299, Loss: 129383.34375, Lr: 0.0001\n",
      "Epoch: 29, Step: 399, Loss: 132785.265625, Lr: 0.0001\n",
      "Epoch: 29, Step: 499, Loss: 128339.046875, Lr: 0.0001\n",
      "Epoch: 29, Step: 599, Loss: 124072.8984375, Lr: 0.0001\n",
      "Epoch: 29, Step: 699, Loss: 128766.171875, Lr: 0.0001\n",
      "Saving checkpoint...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Iterable,Optional\n",
    "import math\n",
    "import torch\n",
    "#import torch.multiprocessing\n",
    "#torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "import torchvision\n",
    "import argparse\n",
    "import os\n",
    "#import timm\n",
    "#from timm.utils import accuracy\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from util import misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Variational Auto-Encoder Example\")\n",
    "    parser.add_argument('--batch_size',default=32,type=int,help='Batch size per GPU (effective batch size is batch_size*accum_iter* #gpus)')\n",
    "    parser.add_argument('--epochs',default=20,type=int)\n",
    "    parser.add_argument('--accum_iter',default=1,type=int)\n",
    "    #Model parameters\n",
    "    parser.add_argument('--image_size', type=int, default=128 , metavar='N', help='Image size')\n",
    "    parser.add_argument('--z_dim', type=int, default=128, metavar='N', help='the dim of latent variable z(default: 20)')\n",
    "\n",
    "    parser.add_argument('--input_channel', type=int, default=3, metavar='N', help='input channel(default: 1 for MNIST)')\n",
    "\n",
    "    #Optimizer parameters\n",
    "    parser.add_argument('--weight_decay',type=float,default=0.0001)\n",
    "    parser.add_argument('--lr',type=float,default=0.0001,metavar='LR')\n",
    "    parser.add_argument('--root_path',default='D:\\\\jiao\\\\datasets\\\\celeba')\n",
    "    parser.add_argument('--output_dir',default='./output_dir_pretrained',help='path to save,empty for no saving')\n",
    "    parser.add_argument('--log_dir',default='./output_dir_pretrained',help='path to tensorboard log')\n",
    "    \n",
    "    parser.add_argument('--resume',default='',help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch',default=0,type=int,metavar='N')\n",
    "    parser.add_argument('--num_workers',default=5,type=int)\n",
    "    parser.add_argument('--pin_mem',action='store_true')\n",
    "    parser.add_argument('--no_pin_mem',action='store_false',dest='pin_mem')\n",
    "    parser.set_defaults(pin_mem=True)\n",
    "    return parser\n",
    "'''创建预处理的transform'''\n",
    "def build_transform(is_train,args):\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.CenterCrop(168),\n",
    "        torchvision.transforms.Resize((args.image_size,args.image_size)),\n",
    "        torchvision.transforms.ToTensor()\n",
    "    ])\n",
    " \n",
    "'''创建数据集 返回dataset'''\n",
    "def build_dataset(is_train,args):\n",
    "    transform = build_transform(is_train,args)\n",
    "    path = os.path.join(args.root_path,'train' if is_train else 'test')\n",
    "    dataset = torchvision.datasets.ImageFolder(path,transform= transform)\n",
    "    info = dataset.find_classes(path)\n",
    "    #print(f\"finding classes from {path}: {info[0]}\")\n",
    "    print(f\"mapping classes from {path} to indexes:{info[1]}\")\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def vae_loss(x_hat, x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Calculate the loss. Note that the loss includes two parts.\n",
    "    :param x_hat:\n",
    "    :param x:\n",
    "    :param mu:\n",
    "    :param log_var:\n",
    "    :return: total loss, BCE and KLD of our model\n",
    "    \"\"\"\n",
    "    # 1. the reconstruction loss. 重建损失\n",
    "    # We regard the MNIST as binary classification\n",
    "    #BCE = F.binary_cross_entropy(x_hat, x, reduction='sum')#MINST等二值图像可以用交叉熵\n",
    "    #jtq20240214 非二值图像用均方误差\n",
    "    BCE = F.mse_loss(x_hat , x , reduction='sum')\n",
    "\n",
    "    # 2. KL-divergence KL散度损失\n",
    "    # D_KL(Q(z|X) || P(z)); calculate in closed form as both dist. are Gaussian\n",
    "    # here we assume that \\Sigma is a diagonal matrix, so as to simplify the computation\n",
    "    # D_KL(Q(z|X) || N(0,1)) = 0.5*( -1 - log(sigma^2) + mu^2 + sigma^2)\n",
    "    # log_var = log(sigma^2)\n",
    "    KLD = 0.5 * torch.sum(torch.exp(log_var) + torch.pow(mu, 2) - 1. - log_var)\n",
    "\n",
    "    # 3. total loss 总损失 = 重建损失 + KL散度损失\n",
    "    loss = BCE + KLD \n",
    "    return loss, BCE, KLD\n",
    "''' 验证函数\n",
    "    输入：\n",
    "    输出：\n",
    "'''\n",
    "@torch.no_grad()\n",
    "def evaluate(data_loader,model,device,epoch):\n",
    "    criterion = vae_loss\n",
    "    metric_logger = misc.MetricLogger(delimiter=\" \")\n",
    "    header = 'Test:'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    #test_avg_loss = 0.0\n",
    "    #下面这段话基本等价于 for (images, targets) in data_loader：\n",
    "    for batch in metric_logger.log_every(data_loader,\n",
    "                                         100, #打印间隔\n",
    "                                         header): #标题\n",
    "        images = batch[0]\n",
    "        target = batch[-1]\n",
    "        images = images.to(device,non_blocking=True)\n",
    "        target = target.to(device,non_blocking=True)\n",
    "        # 前向传播\n",
    "        test_x_hat, test_mu, test_log_var = model(images)\n",
    "        \n",
    "        # 损失函数值\n",
    "        test_loss, test_BCE, test_KLD = criterion(test_x_hat, images, test_mu, test_log_var)\n",
    "        #test_avg_loss += test_loss\n",
    "        batch_size = images.shape[0]\n",
    "        metric_logger.update(loss = test_loss.item())\n",
    "        \n",
    "    # 对和求平均，得到每一张图片的平均损失\n",
    "    #test_avg_loss /= len(mnist_test.dataset)    \n",
    "    '''测试随机生成的隐变量'''\n",
    "    # 随机从隐变量的分布中取隐变量\n",
    "    z = torch.randn(32, args.z_dim).to(device)  # 每一行是一个隐变量，总共有batch_size行\n",
    "    z = model.decoder_projection(z)\n",
    "    # reshape\n",
    "    z = torch.reshape(z,(-1, *model.decoder_in_chw))\n",
    "    \n",
    "    # 对隐变量重构\n",
    "    random_res = model.decode(z)\n",
    "    # 保存重构结果\n",
    "    save_image(random_res, f\"{args.output_dir}/random_sampled-{epoch}.png\" )\n",
    "\n",
    "    \n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print('loss {losses.global_avg:.3f}'.format(losses=metric_logger.loss))\n",
    "    return {k:meter.global_avg for k,meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "'''\n",
    "    训练函数\n",
    "'''\n",
    "def train_one_epoch(model:torch.nn.Module,criterion:torch.nn.Module,\n",
    "                    data_loader:Iterable,optimizer:torch.optim.Optimizer,\n",
    "                    device:torch.device,epoch:int,loss_scaler,max_norm: float=0,\n",
    "                    log_writer=None,args=None):\n",
    "    model.train(True)\n",
    "    print_freq = 2\n",
    "    accum_iter = args.accum_iter\n",
    "    #print(\"in train_one_epoch\")\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "    for data_iter_step,(samples,targets) in enumerate(data_loader):\n",
    "        samples = samples.to(device,non_blocking=True)\n",
    "        targets = targets.to(device,non_blocking=True)\n",
    "        \n",
    "        #print(\"input_dim:\",args.input_dim)\n",
    "        #print(\"samples shape:\",samples.shape)\n",
    "        x_hat, mu, log_var = model(samples)\n",
    "        warmup_lr = args.lr\n",
    "        optimizer.param_groups[0][\"lr\"] = warmup_lr\n",
    "        \n",
    "        loss , _ , _ = criterion(x_hat,samples, mu,log_var)\n",
    "        loss /= accum_iter\n",
    "        \n",
    "        loss_scaler(loss,optimizer,clip_grad=max_norm, \n",
    "                    parameters=model.parameters(),create_graph=False,\n",
    "                    update_grad=(data_iter_step+1)%accum_iter == 0) #训练每accum_iter个batch才更新梯度\n",
    "        loss_value = loss.item()\n",
    "        if (data_iter_step+1)%accum_iter == 0:\n",
    "            optimizer.zero_grad()\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"loss is {loss_value}, stopping training\")\n",
    "            sys.exit(1)\n",
    "        if log_writer is not None and (data_iter_step+1)% (accum_iter*100) == 0 :\n",
    "            epoch_1000x = int((data_iter_step/len(data_loader)+epoch)*1000)\n",
    "            log_writer.add_scalar('loss',loss_value,epoch_1000x)\n",
    "            log_writer.add_scalar('lr',warmup_lr,epoch_1000x)\n",
    "            print(f\"Epoch: {epoch}, Step: {data_iter_step}, Loss: {loss}, Lr: {warmup_lr}\")\n",
    "\n",
    "\n",
    "def main(args,mode='train',test_image_path=''):\n",
    "    print(f\"当前mode: {mode}\")\n",
    "    if mode =='train':\n",
    "        #构建批次\n",
    "        dataset_train = build_dataset(is_train=True,args=args)\n",
    "        dataset_val = build_dataset(is_train=False,args=args)\n",
    "        sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "        sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "        data_loader_train = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_train,sampler=sampler_train,\n",
    "            batch_size=args.batch_size,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=args.pin_mem,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        data_loader_val = torch.utils.data.DataLoader(\n",
    "            dataset=dataset_val,sampler=sampler_val,\n",
    "            batch_size=32,\n",
    "            num_workers=args.num_workers,\n",
    "            pin_memory=args.pin_mem,\n",
    "            drop_last=False,\n",
    "        )\n",
    "        \n",
    "        #构建模型\n",
    "        model = VAE(z_dim = args.z_dim,image_size=args.image_size,ch=args.input_channel)\n",
    "        #model = VAE(args.input_dim,args.hid_dim,args.z_dim)\n",
    "        model = model.to(device)\n",
    "        n_parameters = sum([p.numel() for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "        print(f\"number of trainable parameters(M):{n_parameters/1.e6:.2f}\") #f-string保留两位小数{xxx:.2f}\n",
    "        criterion = vae_loss\n",
    "        \n",
    "        #weight_decay就是对损失函数做L2正则化，防止过拟合\n",
    "        optimizer = torch.optim.AdamW(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)     \n",
    "        #用tensorboard记录日志\n",
    "        os.makedirs(args.log_dir,exist_ok=True)\n",
    "        log_writer = SummaryWriter(log_dir=args.log_dir)\n",
    "        #lossScaler用来反传梯度用的\n",
    "        loss_scaler = NativeScaler()\n",
    "        \n",
    "        #读入已有的模型 resume为空字符串 则不会读取，如果传入时pth文件，则会读取原来的模型\n",
    "        #读取进来时args里epoch会+1\n",
    "        misc.load_model(args=args,model_without_ddp=model,optimizer=optimizer,loss_scaler=loss_scaler)\n",
    "        for epoch in range(args.start_epoch,args.epochs): #start_epoch开始训练\n",
    "            print(f\"Epoch {epoch}\")\n",
    "            print(f\"length of data_loader_train is {len(data_loader_train)}\") #几个batch\n",
    "\n",
    "            if epoch % 1 == 0:\n",
    "                print(\"Evaluating...\")\n",
    "                model.eval()\n",
    "                test_stats = evaluate(data_loader_val,model,device,epoch)\n",
    "                print(f\"loss on the {len(dataset_val)} test images {test_stats['loss']:.2f}\")\n",
    "                if log_writer is not None:\n",
    "                    ''' \n",
    "                        add_scalar(tag, scalar_value, global_step=None, walltime=None) \n",
    "                        add_scalar:记录标量函数,参数:\n",
    "                        tag：图的名称 scalar_value：记录的值 global_step：x轴\n",
    "                    '''\n",
    "                    log_writer.add_scalar('perf/test_loss',test_stats['loss'],epoch)\n",
    "                model.train()    \n",
    "            print(\"Training...\")\n",
    "            train_stats = train_one_epoch(\n",
    "                model,criterion,data_loader_train,\n",
    "                optimizer,device,epoch,#epoch+1, #为什么要+1？\n",
    "                loss_scaler,None,\n",
    "                log_writer=log_writer,args=args\n",
    "            )\n",
    "            if args.output_dir:\n",
    "                print(\"Saving checkpoint...\")\n",
    "                misc.save_model(args=args,model=model,model_without_ddp=model,optimizer=optimizer,\n",
    "                               loss_scaler=loss_scaler,epoch=epoch)\n",
    "            #break\n",
    "        \n",
    "           \n",
    "'''main'''      \n",
    "if __name__ == '__main__':\n",
    "#     z_dim = 64 #隐空间维度\n",
    "#     hid_dim = 512 #encoder和decoder中间层的维度\n",
    "#     in_dim = 128 * 128 \n",
    "#     #out_dim = 28*28 #图片维度\n",
    "#     ch = 3\n",
    "#     bs =128\n",
    "#     x = torch.randn(bs,ch,128,128)\n",
    "#     model = VAE(z_dim = z_dim,image_size=128,ch=3)\n",
    "#     re_x, mu, log_var = model(x)\n",
    "#     print(re_x.shape)\n",
    "    args = get_args_parser()\n",
    "    args = args.parse_args(args=['--batch_size','256','--epochs','100','--num_workers','2','--resume','./output_dir_pretrained/checkpoint-29.pth'])\n",
    "#     dataset = build_dataset(is_train=True,args=args)\n",
    "#     print(dataset[1])\n",
    "    main(args = args,mode='train')     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a776e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 32, 128, 256, 3]\n",
      "[16, 32, 128, 256, 3]\n"
     ]
    }
   ],
   "source": [
    "hiddens=[16,32,128,256]\n",
    "print(hiddens)\n",
    "\n",
    "print( hiddens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
